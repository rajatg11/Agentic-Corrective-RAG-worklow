{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build an Agentic Corrective RAG System with LangGraph"
      ],
      "metadata": {
        "id": "A_fZFcme18vK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project will cover a full hands-on workflow and demonstration of how to build an Agentic Corrective RAG (CRAG) System with LangGraph\n",
        "\n",
        "The idea would be to implement the workflow taking inspiration from the [Corrective Retrieval Augmented Generation](https://arxiv.org/pdf/2401.15884) research paper.\n",
        "\n",
        "The main challenge of RAG systems include:\n",
        "\n",
        "- Poor Retrieval can lead to issues in LLM response generation\n",
        "- Bad retrieval or lack of information in the vector database can also lead to out of context or hallucinated answers\n",
        "\n",
        "The idea is to couple a RAG system with a few checks in place and perform web searches if there is a lack of relevant context documents to the given user query."
      ],
      "metadata": {
        "id": "NYBpZTjLnEXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build this as an agentic RAG system by having a specific functionality step as a node in the graph and use LangGraph to implement it. Key steps in the node will include prompts being sent to LLMs to perform specific tasks as seen in the detailed workflow below:\n",
        "\n",
        "![](https://i.imgur.com/oAfXksw.png)\n",
        "\n",
        "\n",
        "### Agentic Corrective RAG System Workflow\n",
        "\n",
        "This project implements an **Agentic Corrective RAG System** that enhances the reliability and precision of responses by combining document grading, aumenting web search, and RAG. The system ensures only high-quality, grounded answers are generated even when the initial context retrieved from the vector database is incomplete or irrelevant.\n",
        "\n",
        "The workflow includes the following components:\n",
        "\n",
        "1. **Document Retrieval and Grading**:\n",
        "   - A user query is first sent to a **Vector Database** to retrieve relevant documents.\n",
        "   - These documents are passed through an **LLM Grader Prompt**:\n",
        "     - The LLM evaluates each document and labels it as either **'yes'** (relevant) or **'no'** (irrelevant) based on its usefulness for answering the query.\n",
        "     - This filtering step ensures that only the most relevant documents are retained for response generation.\n",
        "\n",
        "2. **Dynamic Decision Routing**:\n",
        "   - A **Decision Node** checks the document grading results:\n",
        "     - If **> 50% of retrieved documents are relevant**, the system proceeds with a **standard RAG Prompt**, using only those documents to generate an answer.\n",
        "     - If **<= 50% of retrieved documents are relevant**, the system switches to a **fallback corrective workflow**.\n",
        "\n",
        "3. **Query Rephrasing and Web Search**:\n",
        "   - When retrieved context is inadequate:\n",
        "     - The original query is sent to the LLM with a **Rephrase Prompt** to generate a more search-optimized version of the query.\n",
        "     - The rephrased query is passed to a **Web Search Tool** to retrieve fresh and more relevant context documents from the web.\n",
        "     - These web-retrieved documents are then combined with any relevant retrieved context documents from the Vector DB and used as the final context documents\n",
        "\n",
        "4. **Final Answer Generation**:\n",
        "   - Whether using documents from the vector database or web search, the final step involves sending the query and relevant context documents into the **RAG Prompt**, which instructs the LLM to:\n",
        "     - Use only the given documents to answer the question.\n",
        "     - Avoid making up information or hallucinating unsupported content.\n",
        "\n",
        "This agentic workflow adds an additional layer of control and recovery to the RAG pipeline, ensuring more accurate responses in dynamic and unpredictable retrieval scenarios.\n",
        "\n",
        "\n",
        "\n",
        "___Created By: [Dipanjan (DJ)](https://www.linkedin.com/in/dipanjans/)___\n"
      ],
      "metadata": {
        "id": "aMX5MULw4JXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install OpenAI, Tavily, LangGraph and LangChain dependencies\n"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.20\n",
        "!pip install langchain-openai==0.3.9\n",
        "!pip install langchain-community==0.3.20\n",
        "!pip install langgraph==0.3.18\n",
        "!pip install langchain-tavily==0.1.5"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3632812b-2ac7-4c3d-e333-b375d9a67588"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.3.20\n",
            "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (0.3.47)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.3.20) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.20) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.20) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.20) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.20) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.3.20) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.20) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain==0.3.20) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain==0.3.20) (1.3.1)\n",
            "Downloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.21\n",
            "    Uninstalling langchain-0.3.21:\n",
            "      Successfully uninstalled langchain-0.3.21\n",
            "Successfully installed langchain-0.3.20\n",
            "Collecting langchain-openai==0.3.9\n",
            "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.9) (0.3.47)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.66.3 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.3.9) (1.68.2)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.3.9)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.9) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.9) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai==0.3.9) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai==0.3.9) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.9) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai==0.3.9) (2.3.0)\n",
            "Downloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m897.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.3.9 tiktoken-0.9.0\n",
            "Collecting langchain-community==0.3.20\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (0.3.47)\n",
            "Collecting langchain<1.0.0,>=0.3.21 (from langchain-community==0.3.20)\n",
            "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.20)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.20)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (0.3.18)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community==0.3.20)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.3.20) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.20) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.20)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.20) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.20) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.20) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.3.20) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.20) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community==0.3.20) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community==0.3.20) (2.27.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.20)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.20) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.21-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain, langchain-community\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.20\n",
            "    Uninstalling langchain-0.3.20:\n",
            "      Successfully uninstalled langchain-0.3.20\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.21 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting langgraph==0.3.18\n",
            "  Downloading langgraph-0.3.18-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph==0.3.18) (0.3.47)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph==0.3.18)\n",
            "  Downloading langgraph_checkpoint-2.0.23-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph==0.3.18)\n",
            "  Downloading langgraph_prebuilt-0.1.7-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph==0.3.18)\n",
            "  Downloading langgraph_sdk-0.1.59-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.3.18)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.10.6)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph==0.3.18)\n",
            "  Downloading ormsgpack-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m823.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (3.10.15)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph==0.3.18) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph==0.3.18) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph==0.3.18) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph==0.3.18) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph==0.3.18) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph==0.3.18) (1.3.1)\n",
            "Downloading langgraph-0.3.18-py3-none-any.whl (136 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.5/136.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.23-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.7-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.59-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m799.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.7/223.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.3.18 langgraph-checkpoint-2.0.23 langgraph-prebuilt-0.1.7 langgraph-sdk-0.1.59 ormsgpack-1.9.0\n",
            "Collecting langchain-tavily==0.1.5\n",
            "  Downloading langchain_tavily-0.1.5-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily==0.1.5) (3.11.14)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily==0.1.5) (0.3.21)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from langchain-tavily==0.1.5) (0.3.47)\n",
            "Collecting mypy<2.0.0,>=1.15.0 (from langchain-tavily==0.1.5)\n",
            "  Downloading mypy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily==0.1.5) (1.18.3)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (4.12.2)\n",
            "Requirement already satisfied: mypy_extensions>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from mypy<2.0.0,>=1.15.0->langchain-tavily==0.1.5) (1.0.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-tavily==0.1.5) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain<0.4.0,>=0.3.20->langchain-tavily==0.1.5) (1.3.1)\n",
            "Downloading langchain_tavily-0.1.5-py3-none-any.whl (14 kB)\n",
            "Downloading mypy-1.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mypy, langchain-tavily\n",
            "Successfully installed langchain-tavily-0.1.5 mypy-1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install PyMuPDF for loading PDF documents"
      ],
      "metadata": {
        "id": "c4htsxA5nRiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf==1.25.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ5GbLySbg6x",
        "outputId": "09469c50-c784-4557-a32a-1a2836f3bc44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf==1.25.4\n",
            "  Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install ChromaDB LangChain Wrapper for Vector DB"
      ],
      "metadata": {
        "id": "sBkJtZv9nXKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-chroma==0.2.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQpclHUxXC3x",
        "outputId": "44664756-29ed-454c-9e52-6b84d4c00a03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-chroma==0.2.2\n",
            "  Downloading langchain_chroma-0.2.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /usr/local/lib/python3.11/dist-packages (from langchain-chroma==0.2.2) (0.3.47)\n",
            "Collecting numpy<2.0.0,>=1.22.4 (from langchain-chroma==0.2.2)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0 (from langchain-chroma==0.2.2)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.10.6)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading posthog-3.23.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.31.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.31.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (13.9.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (0.3.18)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-chroma==0.2.2) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.69.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.31.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_proto-1.31.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.52b1)\n",
            "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.29.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.1.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0->langchain-chroma==0.2.2) (0.6.1)\n",
            "Downloading langchain_chroma-0.2.2-py3-none-any.whl (11 kB)\n",
            "Downloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.31.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.52b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.52b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=ea370839a14ca6c3551d128c9fd36b5cf31fc142bacc68848b9aac7fa6d2363a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-proto, numpy, mmh3, humanfriendly, httptools, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, chroma-hnswlib, build, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, langchain-chroma\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.12 httptools-0.6.4 humanfriendly-10.0 kubernetes-32.0.1 langchain-chroma-0.2.2 mmh3-5.1.0 monotonic-1.6 numpy-1.26.4 onnxruntime-1.21.0 opentelemetry-exporter-otlp-proto-common-1.31.1 opentelemetry-exporter-otlp-proto-grpc-1.31.1 opentelemetry-instrumentation-0.52b1 opentelemetry-instrumentation-asgi-0.52b1 opentelemetry-instrumentation-fastapi-0.52b1 opentelemetry-proto-1.31.1 opentelemetry-util-http-0.52b1 overrides-7.7.0 posthog-3.23.0 pypika-0.48.9 pyproject_hooks-1.2.0 starlette-0.46.1 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Open AI API Key"
      ],
      "metadata": {
        "id": "H9c37cLnSrbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
      ],
      "metadata": {
        "id": "cv3JzCEx_PAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a116c7ad-6c33-44e1-9c19-93da33cb719c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Open AI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter Tavily Search API Key\n",
        "\n",
        "Get a free API key from [here](https://tavily.com/#api)"
      ],
      "metadata": {
        "id": "ucWRRI3QztL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
      ],
      "metadata": {
        "id": "mK-1WLzOrJdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f012b9-8203-4715-8554-23401e0eb13b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Tavily Search API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment Variables"
      ],
      "metadata": {
        "id": "1T0s0um5Svfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_KEY\n",
        "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
      ],
      "metadata": {
        "id": "x1YSuHNF_lbh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a Search Index for Research Paper Data\n",
        "\n",
        "We will build a vector database for retrieval and search by indexing a few research paper documents, similar to any standard RAG workflows"
      ],
      "metadata": {
        "id": "eXI8uXDtcsCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Open AI Embedding Models\n",
        "\n",
        "LangChain enables us to access Open AI embedding models which include the newest models: a smaller and highly efficient `text-embedding-3-small` model, and a larger and more powerful `text-embedding-3-large` model."
      ],
      "metadata": {
        "id": "M8nHAP7XOGOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# details here: https://openai.com/blog/new-embedding-models-and-api-updates\n",
        "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-small')"
      ],
      "metadata": {
        "id": "jzrIVI2NAHC1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the research paper data"
      ],
      "metadata": {
        "id": "RA_-hzHbFeSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you can't download using the following code\n",
        "# go to https://drive.google.com/file/d/1ZOtPmuR-2KpzPvkiQiTVxAyJFo6NszG-/view?usp=sharing download it\n",
        "# manually upload it on colab\n",
        "\n",
        "!gdown 1ZOtPmuR-2KpzPvkiQiTVxAyJFo6NszG-"
      ],
      "metadata": {
        "id": "RZFMYH-yFhWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36a34aa-f821-41ed-ea1f-2ecababd680c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZOtPmuR-2KpzPvkiQiTVxAyJFo6NszG-\n",
            "To: /content/research_papers.zip\n",
            "\r  0% 0.00/19.3M [00:00<?, ?B/s]\r 57% 11.0M/19.3M [00:00<00:00, 55.1MB/s]\r100% 19.3M/19.3M [00:00<00:00, 78.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip research_papers.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKBiOapobqB9",
        "outputId": "9b9d7e05-7536-449d-c679-d8e97cd28c75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  research_papers.zip\n",
            "   creating: research_papers/\n",
            "  inflating: research_papers/attention.pdf  \n",
            "  inflating: research_papers/chain_of_thought.pdf  \n",
            "  inflating: research_papers/diffusion.pdf  \n",
            "  inflating: research_papers/dino.pdf  \n",
            "  inflating: research_papers/peft.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Chunk Documents"
      ],
      "metadata": {
        "id": "4_ReSz-3PVwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a directory loader to use a PDF loader (using pymupdf) and load all PDF documents from a given folder"
      ],
      "metadata": {
        "id": "k8h-eIek5Hn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "# Define a function to create a DirectoryLoader for a specific file type\n",
        "def create_directory_loader(file_type, directory_path, loader_class, loader_args):\n",
        "    return DirectoryLoader(\n",
        "        path=directory_path,\n",
        "        glob=f\"**/*{file_type}\",\n",
        "        loader_cls=loader_class,\n",
        "        loader_kwargs=loader_args,\n",
        "        show_progress=True\n",
        "    )"
      ],
      "metadata": {
        "id": "UXxybBRKY5M7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "pdf_extn = '.pdf'\n",
        "pdf_loader_class = PyMuPDFLoader\n",
        "pdf_loader_args = {} # in case you want to change any settings in pymupdfloader\n",
        "directory= './research_papers'\n",
        "\n",
        "pdf_loader = create_directory_loader(file_type=pdf_extn,\n",
        "                                     directory_path=directory,\n",
        "                                     loader_class=pdf_loader_class,\n",
        "                                     loader_args=pdf_loader_args)\n",
        "\n",
        "# load docs\n",
        "docs = pdf_loader.load()\n",
        "len(docs) # PyMuPDF loads every document and breaks it per page by default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_eUpM5ecaaj",
        "outputId": "35359c82-4ffb-49e6-9c74-0b9eed577db8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E_Iqj4FdfKn",
        "outputId": "abd00714-f665-4501-bdf9-9b50ea6c24c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115848', 'source': 'research_papers/dino.pdf', 'file_path': 'research_papers/dino.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115848', 'page': 0}, page_content='Emerging Properties in Self-Supervised Vision Transformers\\nMathilde Caron1,2\\nHugo Touvron1,3\\nIshan Misra1\\nHerv´e Jegou1\\nJulien Mairal2\\nPiotr Bojanowski1\\nArmand Joulin1\\n1 Facebook AI Research\\n2 Inria∗\\n3 Sorbonne University\\nFigure 1: Self-attention from a Vision Transformer with 8 × 8 patches trained with no supervision. We look at the self-attention of\\nthe [CLS] token on the heads of the last layer. This token is not attached to any label nor supervision. These maps show that the model\\nautomatically learns class-speciﬁc features leading to unsupervised object segmentations.\\nAbstract\\nIn this paper, we question if self-supervised learning pro-\\nvides new properties to Vision Transformer (ViT) [19] that\\nstand out compared to convolutional networks (convnets).\\nBeyond the fact that adapting self-supervised methods to this\\narchitecture works particularly well, we make the follow-\\ning observations: ﬁrst, self-supervised ViT features contain\\nexplicit information about the semantic segmentation of an\\nimage, which does not emerge as clearly with supervised\\nViTs, nor with convnets. Second, these features are also ex-\\ncellent k-NN classiﬁers, reaching 78.3% top-1 on ImageNet\\nwith a small ViT. Our study also underlines the importance of\\nmomentum encoder [33], multi-crop training [10], and the\\nuse of small patches with ViTs. We implement our ﬁndings\\ninto a simple self-supervised method, called DINO, which\\nwe interpret as a form of self-distillation with no labels.\\nWe show the synergy between DINO and ViTs by achieving\\n80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\\n∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000\\nGrenoble, France.\\nCorrespondence: mathilde@fb.com\\nCode: https://github.com/facebookresearch/dino\\n1. Introduction\\nTransformers [70] have recently emerged as an alternative\\nto convolutional neural networks (convnets) for visual recog-\\nnition [19, 69, 83]. Their adoption has been coupled with\\na training strategy inspired by natural language processing\\n(NLP), that is, pretraining on large quantities of data and\\nﬁnetuning on the target dataset [18, 55]. The resulting Vision\\nTransformers (ViT) [19] are competitive with convnets but,\\nthey have not yet delivered clear beneﬁts over them: they\\nare computationally more demanding, require more training\\ndata, and their features do not exhibit unique properties.\\nIn this paper, we question whether the muted success of\\nTransformers in vision can be explained by the use of super-\\nvision in their pretraining. Our motivation is that one of the\\nmain ingredients for the success of Transformers in NLP was\\nthe use of self-supervised pretraining, in the form of close\\nprocedure in BERT [18] or language modeling in GPT [55].\\nThese self-supervised pretraining objectives use the words\\nin a sentence to create pretext tasks that provide a richer\\nlearning signal than the supervised objective of predicting\\na single label per sentence. Similarly, in images, image-\\nlevel supervision often reduces the rich visual information\\ncontained in an image to a single concept selected from a\\npredeﬁned set of a few thousand categories of objects [60].\\nWhile the self-supervised pretext tasks used in NLP are\\n1\\narXiv:2104.14294v2  [cs.CV]  24 May 2021')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use standard recursive character text chunking"
      ],
      "metadata": {
        "id": "3sfyFql6nqae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunk docs\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=300)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "WwLEBC4nF9ly"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunked_docs)"
      ],
      "metadata": {
        "id": "G4E1zYFSG7J-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28a2a77-69e8-4899-d2c3-1d348f3558bb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_docs[:3]"
      ],
      "metadata": {
        "id": "aSbhERAyGw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd9eed4-87b1-4047-afa7-a0438b08d63f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115848', 'source': 'research_papers/dino.pdf', 'file_path': 'research_papers/dino.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115848', 'page': 0}, page_content='Emerging Properties in Self-Supervised Vision Transformers\\nMathilde Caron1,2\\nHugo Touvron1,3\\nIshan Misra1\\nHerv´e Jegou1\\nJulien Mairal2\\nPiotr Bojanowski1\\nArmand Joulin1\\n1 Facebook AI Research\\n2 Inria∗\\n3 Sorbonne University\\nFigure 1: Self-attention from a Vision Transformer with 8 × 8 patches trained with no supervision. We look at the self-attention of\\nthe [CLS] token on the heads of the last layer. This token is not attached to any label nor supervision. These maps show that the model\\nautomatically learns class-speciﬁc features leading to unsupervised object segmentations.\\nAbstract\\nIn this paper, we question if self-supervised learning pro-\\nvides new properties to Vision Transformer (ViT) [19] that\\nstand out compared to convolutional networks (convnets).\\nBeyond the fact that adapting self-supervised methods to this\\narchitecture works particularly well, we make the follow-\\ning observations: ﬁrst, self-supervised ViT features contain\\nexplicit information about the semantic segmentation of an\\nimage, which does not emerge as clearly with supervised\\nViTs, nor with convnets. Second, these features are also ex-\\ncellent k-NN classiﬁers, reaching 78.3% top-1 on ImageNet\\nwith a small ViT. Our study also underlines the importance of\\nmomentum encoder [33], multi-crop training [10], and the\\nuse of small patches with ViTs. We implement our ﬁndings\\ninto a simple self-supervised method, called DINO, which\\nwe interpret as a form of self-distillation with no labels.\\nWe show the synergy between DINO and ViTs by achieving\\n80.1% top-1 on ImageNet in linear evaluation with ViT-Base.\\n∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000\\nGrenoble, France.\\nCorrespondence: mathilde@fb.com\\nCode: https://github.com/facebookresearch/dino\\n1. Introduction\\nTransformers [70] have recently emerged as an alternative\\nto convolutional neural networks (convnets) for visual recog-\\nnition [19, 69, 83]. Their adoption has been coupled with\\na training strategy inspired by natural language processing\\n(NLP), that is, pretraining on large quantities of data and\\nﬁnetuning on the target dataset [18, 55]. The resulting Vision\\nTransformers (ViT) [19] are competitive with convnets but,\\nthey have not yet delivered clear beneﬁts over them: they\\nare computationally more demanding, require more training\\ndata, and their features do not exhibit unique properties.\\nIn this paper, we question whether the muted success of\\nTransformers in vision can be explained by the use of super-\\nvision in their pretraining. Our motivation is that one of the\\nmain ingredients for the success of Transformers in NLP was\\nthe use of self-supervised pretraining, in the form of close\\nprocedure in BERT [18] or language modeling in GPT [55].\\nThese self-supervised pretraining objectives use the words\\nin a sentence to create pretext tasks that provide a richer\\nlearning signal than the supervised objective of predicting\\na single label per sentence. Similarly, in images, image-\\nlevel supervision often reduces the rich visual information\\ncontained in an image to a single concept selected from a\\npredeﬁned set of a few thousand categories of objects [60].\\nWhile the self-supervised pretext tasks used in NLP are\\n1\\narXiv:2104.14294v2  [cs.CV]  24 May 2021'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115848', 'source': 'research_papers/dino.pdf', 'file_path': 'research_papers/dino.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115848', 'page': 1}, page_content='text speciﬁc, many existing self-supervised methods have\\nshown their potential on images with convnets [10, 12, 30,\\n33]. They typically share a similar structure but with differ-\\nent components designed to avoid trivial solutions (collapse)\\nor to improve performance [16]. In this work, inspired from\\nthese methods, we study the impact of self-supervised pre-\\ntraining on ViT features. Of particular interest, we have\\nidentiﬁed several interesting properties that do not emerge\\nwith supervised ViTs, nor with convnets:\\n• Self-supervised ViT features explicitly contain the\\nscene layout and, in particular, object boundaries, as\\nshown in Figure 1. This information is directly accessi-\\nble in the self-attention modules of the last block.\\n• Self-supervised ViT features perform particularly well\\nwith a basic nearest neighbors classiﬁer (k-NN) without\\nany ﬁnetuning, linear classiﬁer nor data augmentation,\\nachieving 78.3% top-1 accuracy on ImageNet.\\nThe emergence of segmentation masks seems to be a\\nproperty shared across self-supervised methods. However,\\nthe good performance with k-NN only emerge when com-\\nbining certain components such as momentum encoder [33]\\nand multi-crop augmentation [10]. Another ﬁnding from our\\nstudy is the importance of using smaller patches with ViTs\\nto improve the quality of the resulting features.\\nOverall, our ﬁndings about the importance of these\\ncomponents lead us to design a simple self-supervised ap-\\nproach that can be interpreted as a form of knowledge\\ndistillation [35] with no labels. The resulting framework,\\nDINO, simpliﬁes self-supervised training by directly pre-\\ndicting the output of a teacher network—built with a mo-\\nmentum encoder—by using a standard cross-entropy loss.\\nInterestingly, our method can work with only a centering\\nand sharpening of the teacher output to avoid collapse, while\\nother popular components such as predictor [30], advanced\\nnormalization [10] or contrastive loss [33] add little beneﬁts\\nin terms of stability or performance. Of particular impor-\\ntance, our framework is ﬂexible and works on both convnets\\nand ViTs without the need to modify the architecture, nor\\nadapt internal normalizations [58].\\nWe further validate the synergy between DINO and ViT\\nby outperforming previous self-supervised features on the\\nImageNet linear classiﬁcation benchmark with 80.1% top-1\\naccuracy with a ViT-Base with small patches. We also con-\\nﬁrm that DINO works with convnets by matching the state\\nof the art with a ResNet-50 architecture. Finally, we discuss\\ndifferent scenarios to use DINO with ViTs in case of limited\\ncomputation and memory capacity. In particular, training\\nDINO with ViT takes just two 8-GPU servers over 3 days\\nto achieve 76.1% on ImageNet linear benchmark, which\\noutperforms self-supervised systems based on convnets of\\ncomparable sizes with signiﬁcantly reduced compute require-\\nments [10, 30].\\nstudent gθs\\nx\\nx2\\nx1\\nteacher gθt\\ncentering\\nsg\\nsoftmax\\np1\\np2\\nsoftmax\\nloss:  \\n- p2 log p1\\nema\\nFigure 2: Self-distillation with no labels. We illustrate DINO in\\nthe case of one single pair of views (x1, x2) for simplicity. The\\nmodel passes two different random transformations of an input\\nimage to the student and teacher networks. Both networks have\\nthe same architecture but different parameters. The output of the\\nteacher network is centered with a mean computed over the batch.\\nEach networks outputs a K dimensional feature that is normalized\\nwith a temperature softmax over the feature dimension. Their\\nsimilarity is then measured with a cross-entropy loss. We apply a\\nstop-gradient (sg) operator on the teacher to propagate gradients\\nonly through the student. The teacher parameters are updated with\\nan exponential moving average (ema) of the student parameters.\\n2. Related work\\nSelf-supervised learning.\\nA large body of work on self-\\nsupervised learning focuses on discriminative approaches\\ncoined instance classiﬁcation [12, 20, 33, 73], which con-\\nsiders each image a different class and trains the model'),\n",
              " Document(metadata={'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20250321115848', 'source': 'research_papers/dino.pdf', 'file_path': 'research_papers/dino.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': 'D:20250321115848', 'page': 1}, page_content='2. Related work\\nSelf-supervised learning.\\nA large body of work on self-\\nsupervised learning focuses on discriminative approaches\\ncoined instance classiﬁcation [12, 20, 33, 73], which con-\\nsiders each image a different class and trains the model\\nby discriminating them up to data augmentations. How-\\never, explicitly learning a classiﬁer to discriminate be-\\ntween all images [20] does not scale well with the num-\\nber of images.\\nWu et al. [73] propose to use a noise\\ncontrastive estimator (NCE) [32] to compare instances in-\\nstead of classifying them. A caveat of this approach is\\nthat it requires comparing features from a large number\\nof images simultaneously. In practice, this requires large\\nbatches [12] or memory banks [33, 73]. Several variants\\nallow automatic grouping of instances in the form of cluster-\\ning [2, 8, 9, 36, 42, 74, 80, 85].\\nRecent works have shown that we can learn unsupervised\\nfeatures without discriminating between images. Of par-\\nticular interest, Grill et al. [30] propose a metric-learning\\nformulation called BYOL, where features are trained by\\nmatching them to representations obtained with a momentum\\nencoder. Methods like BYOL work even without a momen-\\ntum encoder, at the cost of a drop of performance [16, 30].\\nSeveral other works echo this direction, showing that one\\ncan match more elaborate representations [26, 27], train fea-\\ntures matching them to a uniform distribution [6] or by using\\nwhitening [23, 81]. Our approach takes its inspiration from\\nBYOL but operates with a different similarity matching loss')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Vector DB and persist on disk\n",
        "\n",
        "Here we initialize a connection to a Chroma vector DB client, and also we want to save to disk, so we simply initialize the Chroma client and pass the directory where we want the data to be saved to."
      ],
      "metadata": {
        "id": "-PnV9lAXZw9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "# create vector DB of docs and embeddings - takes < 30s on Colab\n",
        "chroma_db = Chroma.from_documents(documents=chunked_docs,\n",
        "                                  collection_name='rag_db',\n",
        "                                  embedding=openai_embed_model,\n",
        "                                  # need to set the distance function to cosine else it uses euclidean by default\n",
        "                                  # check https://docs.trychroma.com/guides#changing-the-distance-function\n",
        "                                  collection_metadata={\"hnsw:space\": \"cosine\"},\n",
        "                                  persist_directory=\"./rag_db\")"
      ],
      "metadata": {
        "id": "kRYfcrsHUxyZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup a Vector Database Retriever\n",
        "\n",
        "Here we use the following retrieval strategy:\n",
        "\n",
        "- Similarity with Threshold Retrieval\n"
      ],
      "metadata": {
        "id": "bprZC4S6TLfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity with Threshold Retrieval\n",
        "\n",
        "We use cosine similarity here and retrieve the top 5 similar documents based on the user input query and also introduce a cutoff to not return any documents which are below a certain similarity threshold"
      ],
      "metadata": {
        "id": "8foBD2xmCDYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_threshold_retriever = chroma_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
        "                                                        search_kwargs={\"k\": 5,\n",
        "                                                                       \"score_threshold\": 0.35})"
      ],
      "metadata": {
        "id": "6ROSNwqeCMRS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test out a few queries"
      ],
      "metadata": {
        "id": "ucrU4weQoKYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is PEFT?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "topk_docs"
      ],
      "metadata": {
        "id": "Nv93k_QpCZv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931bbdb7-80c9-456c-c074-dbccc9dd6a1f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='8449623b-7b02-4119-b3db-2c554f136b4a', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 0, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='keeping the rest unaltered. Furthermore, the application of\\nPEFT extends beyond the realm of NLP and quickly attracts\\ninterest in the CV community for handling fine-tuning vision\\nmodels with large parameters, such as Vision Transformers\\n(ViT) and diffusion models, as well as disciplinary models\\nsuch as vision-language models.\\nIn this survey, we systematically review and categorize\\nrecent advancements in PEFT algorithms as well as the system\\nimplementation costs associated with various PEFT algorithms\\nacross diverse scenarios. Figure 1 presents the overview con-\\ntent for this survey. In section II, we present some fundamental\\nconcepts for LLM and PEFT, including computational flow\\nfor LLM, basic knowledge of PEFT, commonly used datasets\\nand tasks, and evaluation benchmarks. We categorize all\\ntypes of PEFT algorithms in Section III according to their\\ncomputational flow. In Section III-A, we detail additive algo-\\nrithms that either introduce new weight parameters or modify\\nactivations. Algorithms that only require fine-tuning of existing\\nparameters are categorized as selective approaches, which are\\nintroduced in Section III-B. In Section III-C, we explore\\nreparameterized PEFT, which constructs a (low- dimensional)\\nreparameterization of original model parameters for training\\nwhile transforming the weights back to maintain the inference\\nspeed. Additionally, there exist algorithms that combine the\\nabove techniques, and we have classified these as hybrid\\napproaches, elaborating on them in Section III-D. We also\\ninvestigate strategies for further reducing the computational\\ncomplexity of different PEFT algorithms, including KV-cache\\nmanagement, pruning, quantization, and memory optimization,\\nin Section IV.\\nIn Section V, we expand the scope of this survey beyond\\nthe computational perspective to involve various potential\\napplication scenarios. Specifically, we explore innovations that\\narXiv:2403.14608v7  [cs.LG]  16 Sep 2024'),\n",
              " Document(id='9e6dd2c3-5b1e-4b11-b765-e67b04e6a394', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 18, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='mark for PEFT is still lacking. This gap hinders the ability\\nto fairly compare the performance and efficiency of different\\nPEFT approaches. A well-accepted, up-to-date benchmark\\nakin to MMDetection [255] for object detection would enable\\nresearchers to validate their methods against a standard set\\nof tasks and metrics, fostering innovation and collaboration\\nwithin the community.\\nC. Enhance training efficiency\\nThe presumed parameter efficiency of PEFT is not always\\nconsistent with computational and memory savings during\\ntraining. Given that trainable parameters are intertwined within\\nthe pre-trained model’s architecture, computing and storing\\nactivations and gradients for the full model often become\\nnecessary during fine-tuning. This oversight calls for a rethink-\\ning of what constitutes efficiency. As outlined in Section IV,\\npotential solutions lie in the integration of model compres-\\nsion techniques such as pruning and quantization, alongside\\ninnovations specifically designed to optimize memory during\\nPEFT tuning [256]. Further research into enhancing the com-\\nputational efficiency of PEFT methodologies is imperative.\\nD. Explore scaling laws\\nThe design and effectiveness of PEFT methods originally\\ndeveloped for smaller Transformer models do not necessarily\\nscale with larger models. As the size of foundation models\\nincreases, identifying and adapting PEFT strategies that remain\\neffective is crucial. This investigation will aid in customizing\\nPEFT methodologies to suit the evolving landscape of large\\nmodel architectures.\\nE. Serve more models and tasks\\nThe rise of large foundation models across various domains\\npresents new opportunities for PEFT. Designing PEFT meth-\\nods tailored to the unique characteristics of models, such as\\nSora [257], Mamba [258], and LVM [259], can unlock new\\napplication scenarios and opportunities.\\nF. Enhancing data privacy\\nTrusting centralized systems to serve or fine-tune personal-\\nized PEFT modules is yet another issue for system developers.\\nMultiple types of inversion attacks [260], [261] have been pro-\\nposed to reconstruct user’s data by hijacking the intermediate\\nresults. One perspective of future trust-worthy LLM system\\ndesign involves developing an encryption protocol for both\\npersonal data and intermediate training and inference results.'),\n",
              " Document(id='a1b7f60d-2966-458f-9948-bb379897ac3e', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 15, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='content. To overcome this, IP-Adapter introduces a novel\\ndecoupled cross-attention mechanism to distinguish between\\ntext and image features. IP-Adapter adds an additional cross-\\nattention layer exclusively for image features in each cross-\\nattention layer, and only the parameters of the new cross-\\nattention layers are trained.\\nVI. SYSTEM DESIGN CHALLENGE FOR PEFT\\nA. System design for PEFT\\nIn this section, we begin by providing a concise overview\\nof cloud-based PEFT systems and analyzing the design chal-\\nlenges. These include the efficient handling of numerous task-\\nspecific queries via centralized PEFT query servicing, the\\nresolution of privacy and data transmission issues through\\ndistributed PEFT training, and the complexities associated\\nwith concurrent multi-PEFT training processes. Centralized\\nsystems are required to process a substantial volume of queries\\nwith minimal latency and maximal throughput. Distributed\\ntraining frameworks must address privacy concerns and the\\ncomputational inefficiencies that arise from data exchanges\\nbetween users and cloud services. Furthermore, multi-PEFT\\ntraining necessitates the optimization of memory utilization,\\nthe management of simultaneous model training, and the\\nformulation of system architectures capable of supporting\\nmulti-tenant workloads effectively. These challenges under-\\nscore the imperative for innovative approaches to improve\\nscalability, safeguard privacy, and optimize resource allocation\\nin PEFT system architectures. Following this, we present the\\ncorresponding metrics employed for evaluating the system\\nperformance. Furthermore, we delve into three prospective\\nutilization scenarios to illustrate the challenges in system\\ndesign.\\n1) Centralized PEFT Query Serving: Cloud providers have\\nrecently introduced a range of LLM services aimed at pro-\\nviding user applications through application programming\\ninterfaces (APIs) [246], [247]. These APIs facilitate the seam-\\nless integration of many machine-learning functionalities into\\napplications. When receiving one query for one specific down-\\nstream task through API, the cloud-based server processes the\\nquery with one featured LLM model. Under this scenario, the\\nimportance of PEFT becomes apparent. Cloud providers store\\nonly a single copy of the LLM and multiple PEFT modules\\nfeaturing different downstream tasks. This setup allows the\\nLLM to maintain various branches of PEFT modules, each\\nlinked to specific API queries, i.e., PEFT queries.\\nCentralized PEFT query serving solutions address scenarios\\nwhere multiple PEFT queries arrive in quick succession. A\\ncase study of one state-of-the-art system for this purpose\\nis discussed in Section VI-B. Figure 10 (b) illustrates the\\ncomputation pattern for multi-query PEFT inference, wherein\\npacked PEFT queries are scheduled and executed according\\nto their deadlines and current system conditions.\\n2) Distributed PEFT Training: In most cases, personal-\\nized tasks are not fully supported with pre-trained models,'),\n",
              " Document(id='ec8c2b74-026b-41e2-85e3-e26461e65232', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 16, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='17\\nLLMs\\nEdge \\nDevice \\nPersonal data\\nCloud\\nTrainable \\nModules\\n🔥\\nFrozen Large Models\\nScheduler\\nRequest Pool\\nQuery\\nResponse\\nExecution\\nEngine\\nServing System\\nI like\\nI enjoy\\nLLM\\nprogramming\\n(a)\\n(b)\\nFig. 10: (a) Distributed-based system computation pattern; (b)\\ncentralized PEFT Query inference.\\nconsequently, extra fine-tuning is required to be executed\\nwith the methodologies mentioned in the previous sections.\\nHowever, significant concerns arise when considering the\\ntransfer of datasets to cloud providers, given the issues related\\nto data privacy, copyright, proprietary information, and the\\ncomplexities and inefficiencies involved in data transmission.\\nSection VI-C gives two approaches that address this concern.\\n3) Multi-PEFT Training: Different from multiple-PEFT\\nserving, tuning with multiple customized PEFTs always in-\\nvolves different backbone LLMs. Therefore, simultaneously\\ntuning multiple PEFTs can pose considerable challenges.\\nChallenges like how to manage memory gradient and model\\nweights storage, and how to design an efficient kernel for\\nbatching PEFT training remain unsolved. PEFTs will be cat-\\negorized based on their PEFT algorithms and backbone LLM\\nmodels. The design challenge involves how to consolidate\\nmultiple PEFTs with the same LLM backbone and multiple\\ndifferent LLM backbones simultaneously. We present case\\nstudies related to this topic in Section VI-D.\\n4) Evaluation Metrics: For the proposed evaluation met-\\nrics, without loss of generality, we adopt large language\\nmodels as the basis for our metric definitions.\\nTo evaluate the system performance of PEFT serving sys-\\ntems, we propose a set of evaluation metrics:\\n‚ System throughput: Considering PEFT queries as inter\\nand intra tasks, we use tokens per second to measure the\\nsystem throughput.\\n‚ Memory footprint: Run-time memory consumption dur-\\ning query serving, the memory utilization comes from\\nboth model parameters and KV-cache as mentioned in\\nSection IV-A.\\n‚ Accuracy performance: Real-world queries normally\\nhave different context lengths, and performance with\\nvariation length serves as a performance benchmark.\\n‚ Quality of services: Queries are associated with latency\\nrequirements and deadline missing rates are considered\\nas another benchmark.\\nTo assess the efficacy of PEFT training systems, we also\\nestablish a set of evaluative metrics:\\n‚ Accuracy performance: Performance of the fine-tuned\\nmodel over the downstream tasks.\\n‚ Compute cost: The compute cost during forward and\\nbackward propagation operations on cloud servers and\\nedge devices.\\n‚ Communication cost: Refers to the volume of data\\ninvolved during the transfer of intermediate data between\\nthe edge device and the cloud.\\nB. Centralized PEFT Serving Frameworks\\nThe PEFT algorithm is notable for its ability to distin-\\nguish between modifiable and immutable weights within a\\nmodel. This characteristic inspires developers to amalgamate\\ndiverse LLMs with distinct PEFT techniques into collective\\nunits. PetS, as introduced in [248], advocates for a com-\\nprehensive approach to managing multiple PEFT tasks by\\nsuggesting a unified serving framework. The framework’s\\ncore advancement lies in the translation of varying PEFT\\ntasks into integrated computation kernels to enhance efficiency.\\nMoreover, PetS pioneers an orchestrated batching approach\\nand a scheduling methodology, aiming to augment system\\nthroughput and leverage task parallelism respectively.\\nAs depicted in Figure 11, the PetS framework begins\\nwith users registering PEFT tasks through a standardized\\nApplication Programming Interface (API). Upon registration,\\ndevelopers are expected to provide the Pre-Trained Model Tag\\n(e.g., LLaMA), PEFT parameters in a compressed format,\\nand the specific PEFT algorithms (e.g., LoRA, Adapter, Bitfit,\\netc.). These tasks are then endowed with unique identifiers,\\nand the inference engine takes charge of query processing.\\nPetS bifurcates the primary computational workload (e.g.,'),\n",
              " Document(id='449e6537-33fd-4d25-9a01-7f8d4e44909a', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 18, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='19\\nThe second challenge is beyond the computational cost,\\ndesigning an efficient system architecture that can effectively\\nserve multi-tenant PEFT model workloads on the smallest set\\nof GPUs possible while occupying the least amount of GPU\\nresources is another significant challenge. Punica addresses\\nthis by scheduling user requests to active GPUs that already\\nserve or train PEFT models, thereby improving GPU utiliza-\\ntion. For older requests, Punica periodically migrates them to\\nconsolidate workloads, thus freeing up GPU resources for new\\nrequests.\\nb) Multi-Tenant PEFT design: Designing an efficient\\nsystem for the multi-tenant PEFT model serving in the Punica\\nframework focuses on addressing several key challenges to\\nmaximize hardware utilization and minimize resource con-\\nsumption. The system aims to consolidate multi-tenant LoRA\\nserving workloads onto the smallest set of GPUs possible. This\\nconsolidation is achieved through strategic scheduling of user\\nrequests to active GPUs that are already serving or training\\nLoRA models, thereby improving GPU utilization. For older\\nrequests, Punica periodically migrates them to consolidate\\nworkloads further, thus freeing up GPU resources for new\\nrequests. It incorporates on-demand loading of LoRA model\\nweights, which introduces only millisecond-level latency. This\\nfeature provides Punica with the flexibility to dynamically\\nconsolidate user requests to a small set of GPUs, without being\\nconstrained by the specific LoRA models already running on\\nthose GPUs. Besides that, Punica identifies that the decode\\nstage is a predominant factor in the cost of model serving,\\nPunica’s design primarily focuses on optimizing decode stage\\nperformance. Other aspects of model serving leverage straight-\\nforward techniques, such as on-demand loading of LoRA\\nmodel weights, to efficiently manage resource utilization.\\nVII. CONCLUSION AND FUTURE DIRECTIONS\\nIn the current era dominated by large models and large\\ndatasets, PEFT stands out as a highly attractive method for\\nefficiently adapting models to downstream tasks. This tech-\\nnique gains its appeal by addressing the significant challenges\\nposed by traditional full-model fine-tuning, which often places\\nsubstantial computational and data demands. This survey of-\\nfers a comprehensive examination of the most recent advance-\\nments in PEFT, including algorithmic design, computational\\nefficiency, application scenarios, and system implementation\\nfor PEFT. It offers a comprehensive taxonomy and explanation\\nthat serves as an excellent guidance and knowledge base,\\nwhich enables readers of various levels and disciplines to\\nswiftly grasp the core concepts of PEFT.\\nFor further research on PEFT, we propose a series of pos-\\nsible directions from both algorithm and system perspectives,\\nhoping to inspire more researchers to engage in further studies\\nin these areas.\\nA. Simplify hyperparameter tuning\\nThe effectiveness of PEFT is often sensitive to its hyperpa-\\nrameters, such as the bottleneck dimension of the adapter, the\\nrank of LoRA, and the arrangement of various additive PEFT\\nlayers. Manually tuning these hyperparameters will cost lots\\nof effort. Therefore, future efforts could focus on developing\\nmethods that are less dependent on manual tuning of these\\nparameters, or automatically find the optimal configuration\\nsettings. Several studies [82], [83], [84], [98], [99], [100] have\\nstarted to address this issue, but there’s a need for more simple\\nand efficient solutions optimizing these hyperparameters.\\nB. Establish a unified benchmark\\nDespite the existence of libraries like HuggingFace’s\\nPEFT [253] and AdapterHub [254], a comprehensive bench-\\nmark for PEFT is still lacking. This gap hinders the ability\\nto fairly compare the performance and efficiency of different\\nPEFT approaches. A well-accepted, up-to-date benchmark\\nakin to MMDetection [255] for object detection would enable\\nresearchers to validate their methods against a standard set')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like it is getting relevant documents from the vector DB"
      ],
      "metadata": {
        "id": "DIAhCOW3JPCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is time series forecasting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "topk_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds4fuE_4fRx_",
        "outputId": "9aa12adf-3eec-45a6-c7b6-4d650d2d3d43"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.35\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whoops seems like there exists no relevant docs for this. The Agentic Corrective RAG should still be able to handle queries like this"
      ],
      "metadata": {
        "id": "D_TjRvuknG2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are popular patterns for Agentic AI?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "topk_docs"
      ],
      "metadata": {
        "id": "obsI3yKOO0KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a52122-6551-4ea1-c64d-6bb28e01d374"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='653b23c6-1221-462c-b6be-caa1b84ceb8c', metadata={'author': '', 'creationDate': 'D:20250321115731', 'creationdate': 'D:20250321115731', 'creator': 'PDFium', 'file_path': 'research_papers/attention.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 1, 'producer': 'PDFium', 'source': 'research_papers/attention.pdf', 'subject': '', 'title': '', 'total_pages': 10, 'trapped': ''}, page_content='self-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
              " Document(id='981c7209-0e9c-4621-b376-0338bb271cd5', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 19, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='20\\nG. PEFT with model compression\\nModel compression is one of the most effective ways to\\nmake LLM executable on resource-limited devices. Yet, the\\nimpact of model compression techniques on the performance\\nof PEFT algorithms running on hardware remains another\\nsystemic challenge. Common compression techniques such\\nas quantization and pruning necessitate dedicated hardware\\nplatforms to expedite the process, and building such hardware\\nplatforms for compressed models is yet another direction for\\nfuture research.\\nREFERENCES\\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems, vol. 33, pp. 1877–1901, 2020.\\n[2] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A\\ndataset for llm question answering with external tools,” arXiv preprint\\narXiv:2306.13304, 2023.\\n[3] W. Zhu, H. Liu, Q. Dong, J. Xu, L. Kong, J. Chen, L. Li, and S. Huang,\\n“Multilingual machine translation with large language models: Empir-\\nical results and analysis,” arXiv preprint arXiv:2304.04675, 2023.\\n[4] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. Shaikh,\\nN. Akhtar, J. Wu, and S. Mirjalili, “A survey on large language models:\\nApplications, challenges, limitations, and practical usage,” TechRxiv,\\n2023.\\n[5] B. Xu, X. Liu, H. Shen, Z. Han, Y. Li, M. Yue, Z. Peng, Y. Liu, Z. Yao,\\nand D. Xu, “Gentopia: A collaborative platform for tool-augmented\\nllms,” arXiv preprint arXiv:2308.04030, 2023.\\n[6] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem,\\n“Camel: Communicative agents for ”mind” exploration of large lan-\\nguage model society,” in Thirty-seventh Conference on Neural Infor-\\nmation Processing Systems, 2023.\\n[7] Q. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li,\\nL. Jiang, X. Zhang, and C. Wang, “Autogen: Enabling next-gen llm\\napplications via multi-agent conversation framework,” arXiv preprint\\narXiv:2308.08155, 2023.\\n[8] H. Zhang, X. Liu, and J. Zhang, “Summit: Iterative text summarization\\nvia chatgpt,” arXiv preprint arXiv:2305.14835, 2023.\\n[9] B. Zhang and R. Sennrich, “Root mean square layer normalization,”\\nAdvances in Neural Information Processing Systems, vol. 32, 2019.\\n[10] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu, “Roformer:\\nEnhanced transformer with rotary position embedding,” arXiv preprint\\narXiv:2104.09864, 2021.\\n[11] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\\n“Glue: A multi-task benchmark and analysis platform for natural\\nlanguage understanding,” arXiv preprint arXiv:1804.07461, 2018.\\n[12] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of armor\\nconduct electricity? a new dataset for open book question answering,”\\nin EMNLP, 2018.\\n[13] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “Piqa: Reasoning\\nabout physical commonsense in natural language,” in Thirty-Fourth\\nAAAI Conference on Artificial Intelligence, 2020.\\n[14] M. Sap, H. Rashkin, D. Chen, R. LeBras, and Y. Choi, “Socialiqa:\\nCommonsense reasoning about social interactions,” arXiv preprint\\narXiv:1904.09728, 2019.\\n[15] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:\\nCan a machine really finish your sentence?” in Proceedings of the\\n57th Annual Meeting of the Association for Computational Linguistics,\\n2019.\\n[16] C. e. a. Clark, “Boolq: Exploring the surprising difficulty of natural\\nyes/no questions,” in NAACL, 2019.\\n[17] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, “Winogrande:\\nAn adversarial winograd schema challenge at scale,” Communications\\nof the ACM, vol. 64, no. 9, pp. 99–106, 2021.\\n[18] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv:1803.05457v1, 2018.\\n[19] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya-'),\n",
              " Document(id='7f71413f-740c-4b2c-9532-9778c79e04fa', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 14, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='multimodal nature of VLAs. To address this, MaPLe proposes\\nbranch-aware hierarchical prompts that simultaneously adapt\\nboth language and vision branches, and achieves superior\\nperformance. TPT (test-time prompt tuning) [220] studies\\nprompt tuning on the fly without additional training samples.\\nSpecifically, during inference, TPT first augments the input\\nimage into various views, which are then utilized to tune\\nthe learnable prompts. The primary training objective is to\\nensure the VLA can generate consistent responses when faced\\nwith these differing views. A following work DiffTPT [221]\\nfurther enhances the data diversity of test samples through\\ndiffusion models.\\nIn another direction, several studies explore the usage of\\nadapters in VLA. For example, CLIP-Adapter [222] inte-\\ngrates residual-style adapters after CLIP’s text and visual en-\\ncoders. Therefore, unlike CoOp and CoCoOp, CLIP-Adapter\\navoids the gradient backpropagation through CLIP’s encoders,\\nleading to reduced computational requirements in terms of\\nboth training memory and time. Tip-Adapter [223] adopts\\nthe same design with CLIP-Adapter. Different from CLIP-\\nAdapter, the weights of the adapter are obtained in a training-\\nfree manner from a query-key cache model [224], [225]\\nconstructed from few-shot supervisions in a non-parametric\\nmanner. As a result, Tip-Adapter exhibits great efficiency\\ncompared to CLIP-Adapter’s SGD training process.\\nD. PEFT for Diffusion Models\\nDiffusion models [226], [227] are a class of generative\\nmodels that learn to generate data by transforming random\\nnoise into a structured output by a progressive denoising\\nprocess. During training, diffusion models learn to reverse\\nthe noise added to training data using a denoising network,\\nwhile in inference, they start from noise, using a denois-\\ning network to iteratively create data that mirrors the same\\ndistribution as the training examples. Diffusion models have\\nvarious applications [228], [229], [230], [231], [232], while the\\nmost notable is stable diffusion [233], which bridges the gap\\nbetween text and image with its robust capability to generate\\ncoherent and contextually relevant images directly from textual\\ndescriptions. Numerous studies leverage PEFT techniques to\\nadapt a pre-trained diffusion model for downstream tasks, in-\\ncluding accelerating sampling speed [234], [235], text-to-video\\nadaptation [236], [237], text-to-3D adaptation [238], etc. This\\nsection mainly focuses on two scenarios: integrating additional\\ninput modalities beyond mere text-based conditioning, and\\ncustomizing content generation based on pre-trained diffusion\\nmodel.\\n1) Additional Input Control: To incorporate additional in-\\nput modalities (e.g., layout, keypoints) while retaining the'),\n",
              " Document(id='6e2505ad-6650-4413-8441-72b93fe973bc', metadata={'author': '', 'creationDate': 'D:20250321120420', 'creationdate': 'D:20250321120420', 'creator': 'PDFium', 'file_path': 'research_papers/peft.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 6, 'producer': 'PDFium', 'source': 'research_papers/peft.pdf', 'subject': '', 'title': '', 'total_pages': 20, 'trapped': ''}, page_content='7\\nV\\nK\\nQ\\n⊙\\nlk\\nlv\\nlff\\n⊙\\nsoftmax\\nWdown\\n𝛔\\n⊙\\nWup\\n(a) (IA)3\\n⊙\\n⊕\\nOperation 1\\nOperation 2\\n(b) SSF\\nscale\\nshift\\nFig. 6: Illustration of (IA)3 and SSF. Blue represents frozen,\\nwhile yellow represents trainable.\\nmatrices in the self-attention blocks to learn new attention\\npatterns. Besides, APrompt incorporates the learning of a task-\\nspecific head.\\nThe concept of soft prompts has been employed for various\\ndownstream tasks [105], [106], although their training can\\nbe prone to instability and slow convergence. To address\\nthis, SPoT [52] uses a source prompt learned from one or\\nmultiple tasks to initialize prompts for new tasks. Similarly,\\nthe transfer of soft prompts from one task to initialize another\\nis proposed in TPT (transferable prompt tuning) [53], which\\ndemonstrates that a better prompt initialization results in a\\nlarge training convergence speedup. InfoPrompt [54] develops\\ntwo mutual information-based loss functions, i.e., head loss\\nand representation loss, to find better prompt initialization\\nand learn sufficient task-relevant information, thereby also\\nexpediting convergence. PTP [55] delves into the root causes\\nof training instability. It identifies the steep nature of the loss\\nlandscape in conventional prompt tuning, where minor varia-\\ntions in input data can lead to significant loss fluctuations. To\\nmitigate this, PTP introduces perturbation-based regularizers\\nto smooth the loss landscape and consequently stabilize the\\ntraining process. DePT [58] decomposes the soft prompt into\\na shorter soft prompt with a pair of low-rank matrices, which\\nare optimized with two distinct learning rates. This strategy\\nnot only improves performance but also enhances training and\\ninference efficiency. SMoP (Sparse Mixture-of-Prompts) [57]\\nreduce the training and inference cost by utilizing short soft\\nprompts. During training, multiple short soft prompts are\\ntrained, each tailored to specific subsets of the dataset. During\\ninference, SMoP integrates a gating mechanism that routes\\neach input instance to an appropriate short prompt. This\\ntechnique not only increases efficiency in both training and\\ninference stages but also retains performance comparable to\\nthose achieved with longer soft prompts. To further cut down\\nthe number of soft prompt parameters, IPT (Intrinsic Prompt\\nTuning) [56] identifies an intrinsic task subspace by training\\nan auto-encoder on multiple tasks. Tuning on new tasks then\\nrequires adjusting only a few parameters within this subspace,\\nsignificantly reducing the number of training parameters.\\n3) Other Additive Methods: Apart from the methods men-\\ntioned above, there appear other approaches that strategi-\\ncally incorporate additional parameters during the fine-tuning\\nprocess. For example, (IA)3 [59] introduces three learnable\\nrescaling vectors: lk P Rdk, lv P Rdv, and lff P Rdff ,\\nto rescale the key, value, and FFN activations, respectively,\\nas depicted in Figure 6 (a). The operations within the self-\\nattention block can be described as follows:\\nSApxq “ SoftmaxpQplk d KT q\\n?dhead\\nqpplv d V q.\\n(9)\\nIn FFN, the rescaling can be denoted as:\\nFFNT ransfomerpxq “ Wupplff d σpWdownxqq,\\n(10)\\nwhere d is Hadamard product. Furthermore, the scale vectors\\nlk and lv can be seamlessly integrated into the weight matrices\\nof AQ and AW . This integration effectively eliminates the ex-\\ntra computational costs during inference. A similar technique\\nSSF [61] also performs linear transformation to the model\\nactivations, as illustrated in Figure 6 (b). Specifically, after\\neach operation (i.e., MSA, FFN, and layer normalization) in\\nthe pre-trained model, an SSF-ADA layer is injected, which\\nperforms scaling and shifting to the features generated from\\nthe operation. During fine-tuning, only those SSF-ADA layers\\ncan be updated, while during inference, similar to (IA)3, these\\nSSF-ADA layers can be merged into model weights, so no ad-\\nditional inference overhead would be incurred. IPA (Inference-\\nTime Policy Adapters) [62] offers a novel approach to align'),\n",
              " Document(id='f4a78ebe-bf71-4e28-812d-a55937a1eddc', metadata={'author': '', 'creationDate': 'D:20250321120136', 'creationdate': 'D:20250321120136', 'creator': 'PDFium', 'file_path': 'research_papers/diffusion.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 12, 'producer': 'PDFium', 'source': 'research_papers/diffusion.pdf', 'subject': '', 'title': '', 'total_pages': 13, 'trapped': ''}, page_content='2]. It also suggests that large unlabeled datasets could be leveraged in the future to pre-train powerful\\ndiffusion models that can later be improved by using a classiﬁer with desirable properties.\\n8\\nConclusion\\nWe have shown that diffusion models, a class of likelihood-based models with a stationary training\\nobjective, can obtain better sample quality than state-of-the-art GANs. Our improved architecture\\nis sufﬁcient to achieve this on unconditional image generation tasks, and our classiﬁer guidance\\ntechnique allows us to do so on class-conditional tasks. In the latter case, we ﬁnd that the scale\\nof the classiﬁer gradients can be adjusted to trade off diversity for ﬁdelity. These guided diffusion\\nmodels can reduce the sampling time gap between GANs and diffusion models, although diffusion\\nmodels still require multiple forward passes during sampling. Finally, by combining guidance with\\nupsampling, we can further improve sample quality on high-resolution conditional image synthesis.\\n9\\nAcknowledgements\\nWe thank Alec Radford, Mark Chen, Pranav Shyam and Raul Puri for providing feedback on this\\nwork.\\nReferences\\n[1] David Ackley, Geoffrey Hinton, and Terrence Sejnowski. A learning algorithm for boltzmann\\nmachines. Cognitive science, 9(1):147-169, 1985.\\n[2] Adverb.\\nThe\\nbig\\nsleep.\\nhttps://twitter.com/advadnoun/status/\\n1351038053033406468, 2021.\\n[3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973, 2018.\\n[4] Andrew Brock, Theodore Lim, J. M. Ritchie, and Nick Weston. Neural photo editing with\\nintrospective adversarial networks. arXiv:1609.07093, 2016.\\n[5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity\\nnatural image synthesis. arXiv:1809.11096, 2018.\\n[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.\\narXiv:2005.14165, 2020.\\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\\nSutskever.\\nGenerative pretraining from pixels.\\nIn International Conference on Machine\\nLearning, pages 1691–1703. PMLR, 2020.\\n[8] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan.\\nWavegrad: Estimating gradients for waveform generation. arXiv:2009.00713, 2020.\\n[9] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on\\nimages. arXiv:2011.10650, 2021.\\n[10] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz\\nmachine. Neural computation, 7(5):889–904, 1995.\\n[11] Harm de Vries, Florian Strub, Jérémie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron\\nCourville. Modulating early visual processing by language. arXiv:1707.00683, 2017.\\n[12] DeepMind. Biggan-deep 128x128 on tensorﬂow hub. https://tfhub.dev/deepmind/\\nbiggan-deep-128/1, 2018.\\n13')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "None of the above documents are relevant to the query. The Agentic Corrective RAG System should be able to handle this also"
      ],
      "metadata": {
        "id": "I_rmIZQgoBzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create AI Workflows and Tools for Key Components in our Agentic RAG System\n",
        "\n",
        "There are a few AI workflows (sequential pipelines) and tools we would need to create which we will be using in different steps in our Agentic Corrective RAG workflow. These include:\n",
        "\n",
        "- **Query Retrieval Grader Workflow:** This is an essential workflow which can take in a user query, a list of retrieved documents from the vector DB and grade each query as 'yes' or 'no' based on if the document is relevant to the user query or not\n",
        "\n",
        "- **QA RAG Workflow:** This is a workflow which can take in a user query, list of retrived context documents and use a standard RAG workflow where an LLM uses these context documents to generate a contextual response for the user query\n",
        "\n",
        "- **Query Rephraser Workflow:** This is a workflow which uses an LLM to rephrase the given user query (if needed) and make it more optimized for web search\n",
        "\n",
        "- **Web Search Tool:** Build a custom tool which uses the Tavily Search API to search for a user query, get the top web page results and also extract the text content from those web pages\n",
        "\n",
        "Most of these workflows will be built as LangChain chains (pipelines)"
      ],
      "metadata": {
        "id": "vtbO2fC_oPta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Query Retrieval Grader Workflow\n",
        "\n",
        "Here we will use an LLM itself to grade if any retrieved document is relevant to the given user query - Answer will be either `yes` or `no` for each document"
      ],
      "metadata": {
        "id": "RXeilff0c9X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model for LLM output format\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM for grading\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt template for grading\n",
        "SYS_PROMPT = \"\"\"You are an expert grader assessing relevance of a retrieved document to a user question.\n",
        "                Follow these instructions for grading:\n",
        "                  - If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
        "                  - The overall grade should focus more on the semantic meaning rather than just individual words.\n",
        "                  - Your grade should be either 'yes' or 'no' to indicate whether the document is relevant to the question or not.\n",
        "             \"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        (\"human\", \"\"\"Retrieved document:\n",
        "                     {document}\n",
        "\n",
        "                     User question:\n",
        "                     {question}\n",
        "                  \"\"\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Build grader chain\n",
        "doc_grader = (grade_prompt\n",
        "                  |\n",
        "              structured_llm_grader)"
      ],
      "metadata": {
        "id": "ubFlSqlMSU99"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is PEFT?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in topk_docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print('GRADE:', doc_grader.invoke({\"question\": query, \"document\": doc.page_content}))\n",
        "    print()"
      ],
      "metadata": {
        "id": "UuiAlPIbaC0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ff4346-3305-4b2a-f8f2-3a4256a26bca"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keeping the rest unaltered. Furthermore, the application of\n",
            "PEFT extends beyond the realm of NLP and quickly attracts\n",
            "interest in the CV community for handling fine-tuning vision\n",
            "models with large par\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "mark for PEFT is still lacking. This gap hinders the ability\n",
            "to fairly compare the performance and efficiency of different\n",
            "PEFT approaches. A well-accepted, up-to-date benchmark\n",
            "akin to MMDetection [2\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "17\n",
            "LLMs\n",
            "Edge \n",
            "Device \n",
            "Personal data\n",
            "Cloud\n",
            "Trainable \n",
            "Modules\n",
            "🔥\n",
            "Frozen Large Models\n",
            "Scheduler\n",
            "Request Pool\n",
            "Query\n",
            "Response\n",
            "Execution\n",
            "Engine\n",
            "Serving System\n",
            "I like\n",
            "I enjoy\n",
            "LLM\n",
            "programming\n",
            "(a)\n",
            "(b)\n",
            "Fig. 10:\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "content. To overcome this, IP-Adapter introduces a novel\n",
            "decoupled cross-attention mechanism to distinguish between\n",
            "text and image features. IP-Adapter adds an additional cross-\n",
            "attention layer exclus\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "19\n",
            "The second challenge is beyond the computational cost,\n",
            "designing an efficient system architecture that can effectively\n",
            "serve multi-tenant PEFT model workloads on the smallest set\n",
            "of GPUs possible w\n",
            "GRADE: binary_score='yes'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is chain of thought prompting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in topk_docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print('GRADE:', doc_grader.invoke({\"question\": query, \"document\": doc.page_content}))\n",
        "    print()"
      ],
      "metadata": {
        "id": "9YNBjaO4ahnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d4cfb8c-5159-492b-8539-6465b03f44ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the patterns underlying inputs and outputs via a large training dataset).\n",
            "2\n",
            "Chain-of-Thought Prompting\n",
            "Consider one’s own thought process when solving a complicated reasoning task such as a multi-step\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "1\n",
            "Introduction\n",
            "Math Word Problems (GSM8K)\n",
            "0\n",
            "20\n",
            "40\n",
            "60\n",
            "80\n",
            "100\n",
            "33\n",
            "55\n",
            "18\n",
            "57\n",
            "Solve rate (%)\n",
            "Finetuned GPT-3 175B\n",
            "Prior best\n",
            "PaLM 540B: standard prompting\n",
            "PaLM 540B: chain-of-thought prompting\n",
            "Figure 2:\n",
            "PaL\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\n",
            "instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\n",
            "out\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "Chain-of-Thought Prompting Elicits Reasoning\n",
            "in Large Language Models\n",
            "Jason Wei\n",
            "Xuezhi Wang\n",
            "Dale Schuurmans\n",
            "Maarten Bosma\n",
            "Brian Ichter\n",
            "Fei Xia\n",
            "Ed H. Chi\n",
            "Quoc V. Le\n",
            "Denny Zhou\n",
            "Google Research, Brain Te\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\n",
            "reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\n",
            "chai\n",
            "GRADE: binary_score='yes'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are popular patterns for Agentic AI\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in topk_docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print('GRADE:', doc_grader.invoke({\"question\": query, \"document\": doc.page_content}))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRalH6RdgkIq",
        "outputId": "93e524f4-e9b9-44a7-f0b8-9608e6b6c72c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "3\n",
            "Model Architecture\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
            "\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "20\n",
            "G. PEFT with model compression\n",
            "Model compression is one of the most effective ways to\n",
            "make LLM executable on resource-limited devices. Yet, the\n",
            "impact of model compression techniques on the perform\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows\n",
            "chain-of-thought prompting can be improved by taking the majority ﬁnal answer over many sampled\n",
            "generatio\n",
            "GRADE: binary_score='no'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain self-attention in detail\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "for doc in topk_docs:\n",
        "    print(doc.page_content[:200])\n",
        "    print('GRADE:', doc_grader.invoke({\"question\": query, \"document\": doc.page_content}))\n",
        "    print()"
      ],
      "metadata": {
        "id": "8lw0moL0dWjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2f12e8-589b-4907-e9f5-10982d7541be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
            "for different layer types. n is the sequence length, d is the representation dimension, k is the kernel\n",
            "\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            "Multi-Head Attention\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
            "attention layers running in parallel.\n",
            "of the values, w\n",
            "GRADE: binary_score='yes'\n",
            "\n",
            "Emerging Properties in Self-Supervised Vision Transformers\n",
            "Mathilde Caron1,2\n",
            "Hugo Touvron1,3\n",
            "Ishan Misra1\n",
            "Herv´e Jegou1\n",
            "Julien Mairal2\n",
            "Piotr Bojanowski1\n",
            "Armand Joulin1\n",
            "1 Facebook AI Research\n",
            "2 Inria∗\n",
            "\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "7\n",
            "V\n",
            "K\n",
            "Q\n",
            "⊙\n",
            "lk\n",
            "lv\n",
            "lff\n",
            "⊙\n",
            "softmax\n",
            "Wdown\n",
            "𝛔\n",
            "⊙\n",
            "Wup\n",
            "(a) (IA)3\n",
            "⊙\n",
            "⊕\n",
            "Operation 1\n",
            "Operation 2\n",
            "(b) SSF\n",
            "scale\n",
            "shift\n",
            "Fig. 6: Illustration of (IA)3 and SSF. Blue represents frozen,\n",
            "while yellow represents trainable.\n",
            "\n",
            "GRADE: binary_score='no'\n",
            "\n",
            "1\n",
            "Introduction\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
            "in particular, have been firmly established as state of the art approaches in sequence mod\n",
            "GRADE: binary_score='yes'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a QA RAG Workflow\n",
        "\n",
        "This can take in a user query, list of retrived context documents and use a standard RAG workflow where an LLM uses these context documents to generate a contextual response for the user query"
      ],
      "metadata": {
        "id": "Bfn4Z4Em55bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "# Create RAG prompt for response generation\n",
        "prompt = \"\"\"You are an assistant for question-answering tasks on popular research topics.\n",
        "            Use the following pieces of retrieved context to answer the question.\n",
        "            If no context is present or if you don't know the answer, just say that you don't know the answer.\n",
        "            Do not make up the answer unless it is there in the provided context.\n",
        "            Give a detailed answer and to the point answer with regard to the question.\n",
        "\n",
        "            Question:\n",
        "            {question}\n",
        "\n",
        "            Context:\n",
        "            {context}\n",
        "\n",
        "            Answer:\n",
        "         \"\"\"\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
        "\n",
        "# Initialize connection with GPT-4o\n",
        "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0)\n",
        "# Used for separating context docs with new lines\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# create QA RAG chain\n",
        "qa_rag_chain = (\n",
        "    {\n",
        "        \"context\": (itemgetter('context')\n",
        "                        |\n",
        "                    RunnableLambda(format_docs)),\n",
        "        \"question\": itemgetter('question')\n",
        "    }\n",
        "      |\n",
        "    prompt_template\n",
        "      |\n",
        "    chatgpt\n",
        "      |\n",
        "    StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "8Pu5U9HNkl-q"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is chain of thought prompting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "9Wj-MZr2eEq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7ade78-6530-4f81-b944-9bd43cefbe1b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain-of-thought prompting is a method used to enhance the reasoning capabilities of large language models by providing them with a series of intermediate reasoning steps that lead to the final answer of a problem. This approach is inspired by the way humans solve complex reasoning tasks, such as multi-step math word problems, by breaking them down into smaller, manageable steps and solving each one sequentially before arriving at the final solution.\n",
            "\n",
            "In the context of language models, chain-of-thought prompting involves presenting the model with a prompt that includes examples of input, a chain of thought (the intermediate reasoning steps), and the output (the final answer). This method has been shown to significantly improve the performance of language models on tasks that require complex reasoning, such as arithmetic, commonsense, and symbolic reasoning tasks.\n",
            "\n",
            "The effectiveness of chain-of-thought prompting is particularly evident in large language models, where it can lead to substantial improvements in performance compared to standard prompting methods. For example, in experiments with the PaLM 540B model, chain-of-thought prompting achieved state-of-the-art accuracy on the GSM8K benchmark of math word problems, outperforming even finetuned models like GPT-3.\n",
            "\n",
            "One of the key advantages of chain-of-thought prompting is that it does not require a large training dataset or the finetuning of separate model checkpoints for each new task. Instead, it leverages the model's ability to learn from a few examples provided in the prompt, making it a cost-effective and efficient approach to enhancing reasoning in language models. However, it is important to note that while chain-of-thought prompting emulates human reasoning processes, it does not necessarily mean that the neural network is \"reasoning\" in the same way humans do. Additionally, the emergence of chain-of-thought reasoning is more pronounced in larger models, which can be costly to deploy in real-world applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain self-attention in detail\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE1VFHXvhSau",
        "outputId": "c8fa3e93-8c2a-4100-e105-882f8b966e4b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self-attention is a mechanism used in neural networks, particularly in the context of sequence modeling and transduction tasks, such as language translation. It allows the model to weigh the importance of different elements in a sequence when computing a representation for each element. Here's a detailed explanation of how self-attention works and its advantages:\n",
            "\n",
            "### Self-Attention Mechanism\n",
            "\n",
            "1. **Input Representation**: In self-attention, each element of the input sequence is represented as a vector. These vectors are typically derived from embeddings of the input tokens.\n",
            "\n",
            "2. **Queries, Keys, and Values**: For each element in the sequence, self-attention computes three vectors: a query vector, a key vector, and a value vector. These vectors are obtained by multiplying the input vector by learned weight matrices.\n",
            "\n",
            "3. **Scaled Dot-Product Attention**: The core of self-attention is the scaled dot-product attention mechanism. It involves the following steps:\n",
            "   - Compute the dot product of the query vector with all key vectors to get a set of attention scores.\n",
            "   - Scale the attention scores by dividing by the square root of the dimension of the key vectors. This scaling helps in stabilizing the gradients during training.\n",
            "   - Apply a softmax function to the scaled scores to obtain attention weights. These weights determine the importance of each element in the sequence relative to the current element.\n",
            "   - Multiply the attention weights by the corresponding value vectors and sum them up to get the output for the current element.\n",
            "\n",
            "4. **Parallel Computation**: The attention function is computed for all elements in the sequence simultaneously, allowing for efficient parallelization.\n",
            "\n",
            "5. **Multi-Head Attention**: Instead of performing a single attention function, self-attention often uses multi-head attention. This involves projecting the queries, keys, and values into multiple subspaces and performing the attention function in each subspace. The results are then concatenated and linearly transformed. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
            "\n",
            "### Advantages of Self-Attention\n",
            "\n",
            "- **Parallelization**: Unlike recurrent neural networks (RNNs), which process sequences sequentially, self-attention allows for parallel processing of sequence elements, leading to faster computation and training times.\n",
            "\n",
            "- **Long-Range Dependencies**: Self-attention can capture dependencies between distant elements in a sequence more effectively than RNNs, as it does not rely on the sequential processing of elements.\n",
            "\n",
            "- **Path Length**: The maximum path length between any two positions in the input and output sequences is constant (O(1)) in self-attention, compared to O(n) in recurrent layers. This shorter path length facilitates learning long-range dependencies.\n",
            "\n",
            "- **Flexibility**: Self-attention can be used in various tasks beyond sequence transduction, such as reading comprehension, abstractive summarization, and learning task-independent sentence representations.\n",
            "\n",
            "Overall, self-attention is a powerful mechanism that has become a cornerstone of modern neural network architectures, particularly in models like the Transformer, which rely entirely on self-attention to compute representations without using recurrent or convolutional layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the most popular design patterns for Agentic AI?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "i_BK5xvbeYmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06d0ab1-d715-4f6a-ee47-fc8f5c65679e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know the answer. The provided context does not contain information about the most popular design patterns for Agentic AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is time-series forecasting?\"\n",
        "topk_docs = similarity_threshold_retriever.invoke(query)\n",
        "result = qa_rag_chain.invoke(\n",
        "    {\"context\": topk_docs, \"question\": query}\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pinnno3Shb1H",
        "outputId": "771fb9b4-f266-4705-8f04-3ed6c76c99ab"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know the answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Query Rephraser Workflow\n",
        "\n",
        "This uses an LLM to rephrase the given user query (if needed) and make it more optimized for web search"
      ],
      "metadata": {
        "id": "-Fp8Eh0x5bMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM for question rewriting\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompt template for rewriting\n",
        "SYS_PROMPT = \"\"\"Act as a question re-writer and perform the following task:\n",
        "                 - Convert the following input question to a better version that is optimized for web search.\n",
        "                 - Before re-writing, look at the input question and try to reason about the underlying semantic intent / meaning and then re-write it.\n",
        "             \"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", SYS_PROMPT),\n",
        "        (\"human\", \"\"\"Here is the initial question:\n",
        "                     {question}\n",
        "\n",
        "                     Formulate an improved question.\n",
        "                  \"\"\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "# Create rephraser chain\n",
        "question_rewriter = (re_write_prompt\n",
        "                        |\n",
        "                       llm\n",
        "                        |\n",
        "                     StrOutputParser())"
      ],
      "metadata": {
        "id": "Mm7T22tmfYWj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are popular patterns for Agentic AI?\"\n",
        "question_rewriter.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "2bGCpLKzhTUr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6636385a-93f0-4808-8274-93c80096b503"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are the most popular design patterns for Agentic AI systems?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Web Search Tool\n",
        "\n",
        "Here we will be using the [Tavily API](https://tavily.com/#api) for our web searches to search and extract content from web pages in the top search results."
      ],
      "metadata": {
        "id": "howf-v0ARWbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_tavily._utilities import TavilySearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "tavily_search = TavilySearchAPIWrapper()\n",
        "\n",
        "@tool\n",
        "def search_web(query: str) -> list:\n",
        "    \"\"\"Search the web for a query. Userful for general information or general news\"\"\"\n",
        "    results = tavily_search.raw_results(query=query,\n",
        "                                        max_results=6,\n",
        "                                        search_depth='advanced',\n",
        "                                        include_answer=False,\n",
        "                                        include_raw_content=True)\n",
        "    results = [r['raw_content'] for r in results['results']]\n",
        "    results = [doc for doc in results if doc is not None] # remove blank page content\n",
        "    return results"
      ],
      "metadata": {
        "id": "Ue8xgu9WpuPi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Agent State Schema\n",
        "\n",
        "Here we define the key state schema that maintains the agent's state across different steps of execution in the LangGraph workflow.\n",
        "\n",
        "We define a `GraphState` typed dictionary to track relevant information during the execution of the Agentic Corrective RAG System:\n",
        "\n",
        "- **question**: The query asked by the user.\n",
        "- **generation**: The final response generated by the LLM, based on retrieved context documents and / or web search.\n",
        "- **web_search_needed**: A flag (`yes` or `no`) indicating whether a web search fallback is required due to insufficient or irrelevant context from the vector database.\n",
        "- **documents**: A list of context documents used to answer the query. These may be retrieved from either the internal vector database and (or) a web search tool depending on the workflow path.\n"
      ],
      "metadata": {
        "id": "U5tUXzqPsbQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM response generation\n",
        "        web_search_needed: flag of whether to add web search - yes or no\n",
        "        documents: list of context documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    web_search_needed: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "_B2EFrwTpuXB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan the Agent Workflow Structure\n",
        "\n",
        "This is the Agent workflow we will be using\n",
        "\n",
        "![](https://i.imgur.com/xKC0o1A.png)\n",
        "\n",
        "Next up we will define python functions for each of the nodes in this Agent graph"
      ],
      "metadata": {
        "id": "UDbtGknvsJIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Node Functions\n",
        "\n",
        "Each function below represents a stage in processing a user query in the Agentic Corrective RAG System:\n",
        "\n",
        "1. **retrieve**: Retrieves a set of potentially relevant documents from a vector database based on the user's original query.\n",
        "\n",
        "2. **grade_documents**: Uses an LLM-based grading to evaluate whether each retrieved document is relevant to the query. Also sets the `web_search_needed` flag variable.\n",
        "  - Computes the %age of relevant context documents.\n",
        "  - If this is > a threshold (0.5 is our case) then `web_search_needed=No`\n",
        "  - However if this %age of relevant documents is <= the threshold (0.5 in our case) or no documents were retrieved for the query then `web_search_needed=Yes`\n",
        "\n",
        "3. **rewrite_query**: This function rephrases the original query into a version that is better optimized for web search. This ensures better recall during the fallback web search step.\n",
        "\n",
        "4. **web_search**: Performs a web search using the rephrased query and collects external context documents for RAG response generation.\n",
        "\n",
        "5. **generate_answer**: Takes the query and the most relevant set of context documents — whether from the vector DB and / or the web — and generates a grounded, final response using a constrained RAG prompt.\n"
      ],
      "metadata": {
        "id": "ypMMgBg5tQNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the retrieve function\n",
        "\n",
        "Retrieves a set of potentially relevant documents from a vector database based on the user's original query."
      ],
      "metadata": {
        "id": "qXfVLhOWtHJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents - that contains retrieved context documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = similarity_threshold_retriever.invoke(question)\n",
        "    return {\"documents\": documents}"
      ],
      "metadata": {
        "id": "W0rVVBGDpuYw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the grade_documents function\n",
        "\n",
        "Uses an LLM-based grading to evaluate whether each retrieved document is relevant to the query. Also sets the `web_search_needed` flag variable.\n",
        "  - Computes the %age of relevant context documents.\n",
        "  - If this is > a threshold (0.5 is our case) then `web_search_needed=No`\n",
        "  - However if this %age of relevant documents is <= the threshold (0.5 in our case) or no documents were retrieved for the query then `web_search_needed=Yes`"
      ],
      "metadata": {
        "id": "lpOsUnzn6Yo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question\n",
        "    by using an LLM Grader.\n",
        "\n",
        "    If <= 50% documents are relevant to question or documents are empty - Web Search needs to be done\n",
        "    If > 50% documents are relevant to question - Web Search is not needed\n",
        "    Helps filtering out irrelevant documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    RELEVANCE_THRESHOLD = 0.5 # what %age of retrieved documents should at least be relevant\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search_needed = \"No\"\n",
        "    total_irrelevant = 0\n",
        "    if documents:\n",
        "        for d in documents:\n",
        "            score = doc_grader.invoke(\n",
        "                {\"question\": question, \"document\": d.page_content}\n",
        "            )\n",
        "            grade = score.binary_score\n",
        "            if grade == \"yes\":\n",
        "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "                filtered_docs.append(d) # store only relevant documents\n",
        "            else:\n",
        "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "                total_irrelevant += 1 # count number of irrelevant documents\n",
        "\n",
        "        relevance_frac = 1 - (total_irrelevant / len(documents)) # compute %age of relevant documents\n",
        "        if relevance_frac <= RELEVANCE_THRESHOLD:\n",
        "            print(\"---SEVERAL DOCUMENTS (\"+str((1-relevance_frac)*100)+\"%) ARE NOT RELEVANT TO QUESTION - WEB SEARCH NEEDED---\")\n",
        "            web_search_needed = \"Yes\"\n",
        "        else:\n",
        "            print(\"---MOST DOCUMENTS (\"+str(relevance_frac*100)+\"%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\")\n",
        "\n",
        "    else:\n",
        "        print(\"---NO DOCUMENTS RETRIEVED - WEB SEARCH NEEDED---\")\n",
        "        web_search_needed = \"Yes\"\n",
        "\n",
        "    return {\"documents\": filtered_docs, \"web_search_needed\": web_search_needed}"
      ],
      "metadata": {
        "id": "NI20nh1DtTwJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the rewrite_query function\n",
        "\n",
        "This function rephrases the original query into a version that is better optimized for web search. This ensures better recall during the fallback web search step."
      ],
      "metadata": {
        "id": "fj1jk8C16hhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_query(state):\n",
        "    \"\"\"\n",
        "    Rewrite the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased or re-written question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---REWRITE QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"question\": better_question}"
      ],
      "metadata": {
        "id": "Xw_iVhvWuSG-"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the web_search function\n",
        "\n",
        "Performs a web search using the rephrased query and collects external context documents for RAG response generation."
      ],
      "metadata": {
        "id": "HMogEnhT7Icn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-written question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search and extract content from top search results\n",
        "    docs = search_web.invoke(question)\n",
        "    web_results = [Document(page_content=d) for d in docs]\n",
        "    documents.extend(web_results)\n",
        "\n",
        "    return {\"documents\": documents}"
      ],
      "metadata": {
        "id": "YM7f6AyCvUP_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the generate_answer function\n",
        "\n",
        "Takes the query and the most relevant set of context documents — whether from the vector DB and / or the web — and generates a grounded, final response using a constrained RAG prompt."
      ],
      "metadata": {
        "id": "ruTBxSkm7R2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(state):\n",
        "    \"\"\"\n",
        "    Generate answer from context document using LLM\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE ANSWER---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = qa_rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"generation\": generation}"
      ],
      "metadata": {
        "id": "MemqMTolwLhA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and Compile the Agent Graph Workflow\n",
        "\n",
        "We construct a LangGraph agent workflow with the following sequence of nodes:\n",
        "\n",
        "1. **retrieve** → Retrieves documents from a vector database based on the user’s original query.\n",
        "2. **grade_documents** → Uses an LLM grader to assess whether the retrieved documents are relevant to the query.\n",
        "3. If **> 50% documents are relevant**, proceed directly to **generate_answer**.\n",
        "4. If **<= 50% documents are relevant**, route the flow through a corrective fallback:\n",
        "   - **rewrite_query** → Rephrases the original query to improve information retrieval from the web.\n",
        "   - **web_search** → Performs a web search using the rephrased query and collects updated context.\n",
        "5. Finally, **generate_answer** → Uses the best available context (from DB or web) to produce a grounded and accurate response.\n"
      ],
      "metadata": {
        "id": "EpjPx4v89BVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "# Create a typed LangGraph state graph using the custom GraphState\n",
        "agentic_rag = StateGraph(GraphState)\n",
        "\n",
        "# Register each functional node in the graph that represents a step in the agent workflow\n",
        "agentic_rag.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "agentic_rag.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "agentic_rag.add_node(\"rewrite_query\", rewrite_query)  # transform query\n",
        "agentic_rag.add_node(\"web_search\", web_search)  # web search\n",
        "agentic_rag.add_node(\"generate_answer\", generate_answer)  # generate answer\n",
        "\n",
        "# Define the router function that directs the flow based on web_search_needed flag status\n",
        "def generate_or_search(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question for web search.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    web_search_needed = state[\"web_search_needed\"]\n",
        "\n",
        "    if web_search_needed == \"Yes\":\n",
        "        # web search needed as some documents are not relevant\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\")\n",
        "        return \"rewrite_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE RESPONSE---\")\n",
        "        return \"generate_answer\"\n",
        "\n",
        "\n",
        "# Define the flow of transitions between the nodes in the graph\n",
        "\n",
        "# starting point is to retrieve documents from the vector db\n",
        "agentic_rag.set_entry_point(\"retrieve\")\n",
        "# grade documents after that\n",
        "agentic_rag.add_edge(\"retrieve\", \"grade_documents\")\n",
        "# either generate RAG response or go towards rewrite query for web search\n",
        "agentic_rag.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    generate_or_search,\n",
        "    [\"rewrite_query\", \"generate_answer\"]\n",
        ")\n",
        "# web search after rewriting query\n",
        "agentic_rag.add_edge(\"rewrite_query\", \"web_search\")\n",
        "# generate answer after web search\n",
        "agentic_rag.add_edge(\"web_search\", \"generate_answer\")\n",
        "# stop agent after generating answer\n",
        "agentic_rag.add_edge(\"generate_answer\", END)\n",
        "\n",
        "# Compile\n",
        "agentic_rag = agentic_rag.compile()"
      ],
      "metadata": {
        "id": "F24b6qm_yhnE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "display(Image(agentic_rag.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "X3D6GCcN0ElZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "19cbb3da-ff6a-4cf5-9daa-03c082048995"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Agentic CRAG System"
      ],
      "metadata": {
        "id": "pXW3yX7s9MMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is chain of thought prompting?\"\n",
        "response = agentic_rag.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "tgny4kCM0OrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7de3a4-1a45-499b-f548-46015a73f469"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---MOST DOCUMENTS (100.0%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE RESPONSE---\n",
            "---GENERATE ANSWER---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "id": "9Y7Q-KAF2oqA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "3ee6a12d-74ed-405e-d702-5a7e2a7b8785"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Chain-of-thought prompting is a method used to enhance the reasoning capabilities of large language models by providing them with a series of intermediate reasoning steps that lead to the final answer of a problem. This approach is inspired by the way humans solve complex reasoning tasks, such as multi-step math word problems, by breaking them down into smaller, manageable steps and solving each one sequentially.\n\nIn practice, chain-of-thought prompting involves presenting the language model with a prompt that includes examples of input-output pairs, where the output is not just the final answer but also includes a detailed explanation of the reasoning process. This is done by providing a few exemplars that demonstrate the task, which helps the model understand how to approach similar problems.\n\nThe method has been shown to significantly improve the performance of language models on a variety of reasoning tasks, including arithmetic, commonsense, and symbolic reasoning. For instance, experiments have demonstrated that chain-of-thought prompting can lead to state-of-the-art performance on benchmarks like the GSM8K math word problems, outperforming standard prompting techniques.\n\nOne of the key advantages of chain-of-thought prompting is that it does not require extensive training datasets or fine-tuning of the model. Instead, it leverages the model's existing capabilities by providing it with a few examples that illustrate the reasoning process. This makes it a cost-effective and efficient way to enhance the reasoning abilities of large language models. However, it is important to note that while this method emulates human-like reasoning processes, it does not necessarily mean that the model is truly \"reasoning\" in the human sense. Additionally, the effectiveness of chain-of-thought prompting tends to emerge only at larger model scales, which can be costly to implement in real-world applications."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "oVSzCfgb0q4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc87510-634b-412d-e972-f65f0d47eeb3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'what is chain of thought prompting?',\n",
              " 'generation': 'Chain-of-thought prompting is a method used to enhance the reasoning capabilities of large language models by providing them with a series of intermediate reasoning steps that lead to the final answer of a problem. This approach is inspired by the way humans solve complex reasoning tasks, such as multi-step math word problems, by breaking them down into smaller, manageable steps and solving each one sequentially.\\n\\nIn practice, chain-of-thought prompting involves presenting the language model with a prompt that includes examples of input-output pairs, where the output is not just the final answer but also includes a detailed explanation of the reasoning process. This is done by providing a few exemplars that demonstrate the task, which helps the model understand how to approach similar problems.\\n\\nThe method has been shown to significantly improve the performance of language models on a variety of reasoning tasks, including arithmetic, commonsense, and symbolic reasoning. For instance, experiments have demonstrated that chain-of-thought prompting can lead to state-of-the-art performance on benchmarks like the GSM8K math word problems, outperforming standard prompting techniques.\\n\\nOne of the key advantages of chain-of-thought prompting is that it does not require extensive training datasets or fine-tuning of the model. Instead, it leverages the model\\'s existing capabilities by providing it with a few examples that illustrate the reasoning process. This makes it a cost-effective and efficient way to enhance the reasoning abilities of large language models. However, it is important to note that while this method emulates human-like reasoning processes, it does not necessarily mean that the model is truly \"reasoning\" in the human sense. Additionally, the effectiveness of chain-of-thought prompting tends to emerge only at larger model scales, which can be costly to implement in real-world applications.',\n",
              " 'web_search_needed': 'No',\n",
              " 'documents': [Document(id='023d351f-7017-443a-a52c-c87746bc91d9', metadata={'author': '', 'creationDate': 'D:20250321120028', 'creationdate': 'D:20250321120028', 'creator': 'PDFium', 'file_path': 'research_papers/chain_of_thought.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 1, 'producer': 'PDFium', 'source': 'research_papers/chain_of_thought.pdf', 'subject': '', 'title': '', 'total_pages': 9, 'trapped': ''}, page_content='the patterns underlying inputs and outputs via a large training dataset).\\n2\\nChain-of-Thought Prompting\\nConsider one’s own thought process when solving a complicated reasoning task such as a multi-step\\nmath word problem. It is typical to decompose the problem into intermediate steps and solve each\\nbefore giving the ﬁnal answer: “After Jane gives 2 ﬂowers to her mom she has 10 . . . then after she\\ngives 3 to her dad she will have 7 . . . so the answer is 7.” The goal of this paper is to endow language\\nmodels with the ability to generate a similar chain of thought—a coherent series of intermediate\\nreasoning steps that lead to the ﬁnal answer for a problem. We will show that sufﬁciently large\\n2'),\n",
              "  Document(id='42665456-08a5-47cc-9733-6667ce7d918f', metadata={'author': '', 'creationDate': 'D:20250321120028', 'creationdate': 'D:20250321120028', 'creator': 'PDFium', 'file_path': 'research_papers/chain_of_thought.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 1, 'producer': 'PDFium', 'source': 'research_papers/chain_of_thought.pdf', 'subject': '', 'title': '', 'total_pages': 9, 'trapped': ''}, page_content='1\\nIntroduction\\nMath Word Problems (GSM8K)\\n0\\n20\\n40\\n60\\n80\\n100\\n33\\n55\\n18\\n57\\nSolve rate (%)\\nFinetuned GPT-3 175B\\nPrior best\\nPaLM 540B: standard prompting\\nPaLM 540B: chain-of-thought prompting\\nFigure 2:\\nPaLM 540B uses chain-of-\\nthought prompting to achieve new state-\\nof-the-art performance on the GSM8K\\nbenchmark of math word problems.\\nFinetuned GPT-3 and prior best are from\\nCobbe et al. (2021).\\nThe NLP landscape has recently been revolutionized by\\nlanguage models (Peters et al., 2018; Devlin et al., 2019;\\nBrown et al., 2020, inter alia). Scaling up the size of lan-\\nguage models has been shown to confer a range of beneﬁts,\\nsuch as improved performance and sample efﬁciency (Ka-\\nplan et al., 2020; Brown et al., 2020, inter alia). However,\\nscaling up model size alone has not proved sufﬁcient for\\nachieving high performance on challenging tasks such as\\narithmetic, commonsense, and symbolic reasoning (Rae\\net al., 2021).\\nThis work explores how the reasoning ability of large\\nlanguage models can be unlocked by a simple method\\nmotivated by two ideas. First, techniques for arithmetic\\nreasoning can beneﬁt from generating natural language\\nrationales that lead to the ﬁnal answer. Prior work has\\ngiven models the ability to generate natural language inter-\\nmediate steps by training from scratch (Ling et al., 2017)\\nor ﬁnetuning a pretrained model (Cobbe et al., 2021), in\\naddition to neuro-symbolic methods that use formal lan-\\nguages instead of natural language (Roy and Roth, 2015;\\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\\n2019). Second, large language models offer the exciting\\nprospect of in-context few-shot learning via prompting. That is, instead of ﬁnetuning a separate\\nlanguage model checkpoint for each new task, one can simply “prompt” the model with a few\\ninput–output exemplars demonstrating the task. Remarkably, this has been successful for a range of\\nsimple question-answering tasks (Brown et al., 2020).\\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\\nﬁnetuning methods, it is costly to create a large set of high quality rationales, which is much more\\ncomplicated than simple input–output pairs used in normal machine learning. For the traditional few-\\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\\nSpeciﬁcally, we explore the ability of language models to perform few-shot prompting for reasoning\\ntasks, given a prompt that consists of triples: ⟨input, chain of thought, output⟩. A chain of thought is\\na series of intermediate natural language reasoning steps that lead to the ﬁnal output, and we refer to\\nthis approach as chain-of-thought prompting. An example prompt is shown in Figure 1.\\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks,\\nshowing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking\\ndegree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems\\n(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting\\nby a large margin and achieves new state-of-the-art performance. A prompting only approach is\\nimportant because it does not require a large training dataset and because a single model checkpoint\\ncan perform many tasks without loss of generality. This work underscores how large language models\\ncan learn via a few examples with natural language data about the task (c.f. automatically learning\\nthe patterns underlying inputs and outputs via a large training dataset).\\n2\\nChain-of-Thought Prompting\\nConsider one’s own thought process when solving a complicated reasoning task such as a multi-step\\nmath word problem. It is typical to decompose the problem into intermediate steps and solve each'),\n",
              "  Document(id='d33c7772-03fa-4baa-b9a9-d1a12ab6159b', metadata={'author': '', 'creationDate': 'D:20250321120028', 'creationdate': 'D:20250321120028', 'creator': 'PDFium', 'file_path': 'research_papers/chain_of_thought.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 8, 'producer': 'PDFium', 'source': 'research_papers/chain_of_thought.pdf', 'subject': '', 'title': '', 'total_pages': 9, 'trapped': ''}, page_content='et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8\\nConclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9'),\n",
              "  Document(id='58d30f25-915a-4736-95c9-316584372251', metadata={'author': '', 'creationDate': 'D:20250321120028', 'creationdate': 'D:20250321120028', 'creator': 'PDFium', 'file_path': 'research_papers/chain_of_thought.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 0, 'producer': 'PDFium', 'source': 'research_papers/chain_of_thought.pdf', 'subject': '', 'title': '', 'total_pages': 9, 'trapped': ''}, page_content='Chain-of-Thought Prompting Elicits Reasoning\\nin Large Language Models\\nJason Wei\\nXuezhi Wang\\nDale Schuurmans\\nMaarten Bosma\\nBrian Ichter\\nFei Xia\\nEd H. Chi\\nQuoc V. Le\\nDenny Zhou\\nGoogle Research, Brain Team\\n{jasonwei,dennyzhou}@google.com\\nAbstract\\nWe explore how generating a chain of thought—a series of intermediate reasoning\\nsteps—signiﬁcantly improves the ability of large language models to perform\\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\\nnaturally in sufﬁciently large language models via a simple method called chain-of-\\nthought prompting, where a few chain of thought demonstrations are provided as\\nexemplars in prompting.\\nExperiments on three large language models show that chain-of-thought prompting\\nimproves performance on a range of arithmetic, commonsense, and symbolic\\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\\naccuracy on the GSM8K benchmark of math word problems, surpassing even\\nﬁnetuned GPT-3 with a veriﬁer.\\nA: The cafeteria had 23 apples originally. They used \\n20 to make lunch. So they had 23 - 20 = 3. They \\nbought 6 more apples, so they have 3 + 6 = 9. The \\nanswer is 9.\\nChain-of-Thought Prompting\\nQ: Roger has 5 tennis balls. He buys 2 more cans of \\ntennis balls. Each can has 3 tennis balls. How many \\ntennis balls does he have now?\\nA: The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to \\nmake lunch and bought 6 more, how many apples \\ndo they have?\\nA: The answer is 27.\\nStandard Prompting\\nQ: Roger has 5 tennis balls. He buys 2 more cans of \\ntennis balls. Each can has 3 tennis balls. How many \\ntennis balls does he have now?\\nA: Roger started with 5 balls. 2 cans of 3 tennis balls \\neach is 6 tennis balls. 5 + 6 = 11. The answer is 11.\\nQ: The cafeteria had 23 apples. If they used 20 to \\nmake lunch and bought 6 more, how many apples \\ndo they have?\\nModel Input\\nModel Output\\nModel Output\\nModel Input\\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2201.11903v6  [cs.CL]  10 Jan 2023'),\n",
              "  Document(id='d46aa6b4-a07b-4452-984e-78af6c2a87f6', metadata={'author': '', 'creationDate': 'D:20250321120028', 'creationdate': 'D:20250321120028', 'creator': 'PDFium', 'file_path': 'research_papers/chain_of_thought.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': '', 'moddate': '', 'page': 8, 'producer': 'PDFium', 'source': 'research_papers/chain_of_thought.pdf', 'subject': '', 'title': '', 'total_pages': 9, 'trapped': ''}, page_content='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ﬁnetuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfully—in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answers—for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7\\nRelated Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ﬁnal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8')]}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the most popular design patterns for Agentic AI?\"\n",
        "response = agentic_rag.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "aAm5ERMM2fFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5f1976-9f5a-44c7-9ec1-f3d2d28ea849"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---SEVERAL DOCUMENTS (100.0%) ARE NOT RELEVANT TO QUESTION - WEB SEARCH NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\n",
            "---REWRITE QUERY---\n",
            "---WEB SEARCH---\n",
            "---GENERATE ANSWER---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "id": "CTCcEfNx2w-5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "e2f3b8c3-c388-4bbd-e603-88f9196befd6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The top design patterns commonly used in Agentic AI development are:\n\n1. **Reflection Pattern**: This pattern enhances an AI's ability to evaluate and refine its own outputs. It involves the AI reviewing its generated content or code as if it were a human reviewer, identifying errors, gaps, or areas that need improvement, and then offering suggestions for improvement. This iterative self-critique loop can be repeated multiple times to achieve a refined, polished result. It is particularly useful in tasks requiring precision, such as content creation, problem-solving, or code generation.\n\n2. **Tool Use Pattern**: This pattern broadens an AI's capability by allowing it to interact with external tools and resources to enhance its problem-solving abilities. Instead of relying solely on internal computations or knowledge, an AI following this pattern can access databases, search the web, or execute complex functions via programming languages like Python. This enables AI systems to tackle more complex, multifaceted tasks where internal knowledge alone isn’t sufficient.\n\n3. **Planning Pattern**: This pattern enables an AI to break down large, complicated tasks into smaller, more manageable components. It involves creating a roadmap of subtasks and determining the most efficient path to completion. Variations like ReAct (Reasoning and Acting) and ReWOO (Reasoning With Open Ontology) further extend this approach by integrating decision-making and contextual reasoning into the planning process, allowing for more adaptive and flexible planning.\n\n4. **Multi-Agent Pattern**: This pattern involves assigning different agents (instances of an AI model with specific roles or functions) to handle various subtasks. These agents can work independently on their assignments while also communicating and collaborating to achieve a unified outcome. This pattern is particularly powerful for tackling large-scale or complex problems that require diverse skill sets.\n\nThese design patterns push the boundaries of what AI can do by encouraging self-evaluation, tool integration, strategic thinking, and collaboration, making AI systems more autonomous and capable."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain self-attention in detail\"\n",
        "response = agentic_rag.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "0PH45g2E3Dta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090bf93a-7682-4cab-e11f-2934b71bb88f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVAL FROM VECTOR DB---\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---MOST DOCUMENTS (60.0%) ARE RELEVANT TO QUESTION - WEB SEARCH NOT NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE RESPONSE---\n",
            "---GENERATE ANSWER---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "id": "Z6G7ywp53FrO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "ca44037c-5e25-42ba-e4a3-4557ce2fa323"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Self-attention is a mechanism used in neural networks, particularly in models like the Transformer, to compute a representation of a sequence by relating different positions within the same sequence. It allows the model to weigh the importance of different elements in the sequence when computing the representation of each element, enabling the model to capture dependencies regardless of their distance in the sequence.\n\n### Key Components of Self-Attention:\n\n1. **Input Representation**:\n   - The input to a self-attention mechanism consists of queries, keys, and values, which are derived from the input sequence. Each element in the sequence is transformed into these three components.\n\n2. **Scaled Dot-Product Attention**:\n   - The core operation in self-attention is the scaled dot-product attention. It involves computing the dot product of the query with all keys, scaling the result by the square root of the dimension of the keys (to prevent large dot products that could push the softmax function into regions with small gradients), and applying a softmax function to obtain attention weights. These weights are then used to compute a weighted sum of the values, producing the output of the attention mechanism.\n\n   \\[\n   \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n   \\]\n\n   Here, \\(Q\\), \\(K\\), and \\(V\\) are matrices of queries, keys, and values, respectively, and \\(d_k\\) is the dimension of the keys.\n\n3. **Multi-Head Attention**:\n   - Instead of performing a single attention function, multi-head attention projects the queries, keys, and values into multiple subspaces and performs attention in parallel. This allows the model to jointly attend to information from different representation subspaces at different positions.\n\n   - The outputs from each attention head are concatenated and linearly transformed to produce the final output.\n\n4. **Positional Encoding**:\n   - Since self-attention does not inherently capture the order of the sequence, positional encodings are added to the input embeddings to provide information about the position of each element in the sequence. These encodings can be fixed (e.g., using sine and cosine functions) or learned.\n\n### Advantages of Self-Attention:\n\n- **Parallelization**: Unlike recurrent layers, which process sequences sequentially, self-attention allows for parallelization across sequence positions, significantly speeding up computation, especially for long sequences.\n\n- **Long-Range Dependencies**: Self-attention can capture dependencies between distant positions in the sequence with a constant number of operations, making it more effective at learning long-range dependencies compared to recurrent or convolutional layers.\n\n- **Flexibility**: Self-attention is versatile and has been successfully applied to various tasks, including reading comprehension, summarization, and translation, without the need for recurrence or convolution.\n\nIn summary, self-attention is a powerful mechanism that enhances the ability of models to process sequences by focusing on relevant parts of the input, enabling efficient computation and effective learning of complex dependencies."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is time series forecasting?\"\n",
        "response = agentic_rag.invoke({\"question\": query})"
      ],
      "metadata": {
        "id": "KlSvUePF3Hqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39554355-5843-483e-f175-74b33b6fead3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVAL FROM VECTOR DB---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.vectorstores.base:No relevant docs were retrieved using the relevance score threshold 0.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---NO DOCUMENTS RETRIEVED - WEB SEARCH NEEDED---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\n",
            "---REWRITE QUERY---\n",
            "---WEB SEARCH---\n",
            "---GENERATE ANSWER---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response['generation']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "WK8aw84cIWYq",
        "outputId": "b3c2e5e0-a2e5-4649-8650-8efc97f0e2e3"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Time series forecasting is a statistical technique used to predict future values based on historical time-stamped data. It involves analyzing patterns and trends in past data points collected over time to develop a model that can predict those patterns in the future. This method is crucial for making data-driven decisions across various fields, including finance, retail, healthcare, and more.\n\nIn time series forecasting, the data is typically decomposed into several components: trends, seasonality, cycles, and irregularities (or noise). Trends represent long-term movements in data, seasonality refers to regular patterns that occur at specific intervals, cycles are longer-term fluctuations, and irregularities are random variations that cannot be attributed to the other components.\n\nThe process of time series forecasting involves several steps, including data collection, preprocessing (such as handling missing values and outliers), model selection, and evaluation. Common models used in time series forecasting include ARIMA (AutoRegressive Integrated Moving Average), exponential smoothing, and neural networks like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks.\n\nTime series forecasting is widely used in various applications, such as predicting stock prices, sales forecasting, weather forecasting, and resource allocation. It helps organizations anticipate future trends, optimize resources, and make informed strategic decisions. However, it is important to note that time series forecasting is not infallible and is subject to limitations, such as handling non-stationary data, managing complex seasonality, and adapting to unexpected events. Therefore, it requires careful consideration of data quality, model selection, and evaluation to ensure accurate and reliable forecasts."
          },
          "metadata": {}
        }
      ]
    }
  ]
}